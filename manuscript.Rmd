---
title: "Containers and R---the Rockerverse and beyond"
author:
  - name: Daniel Nüst
    affiliation: University of Münster
    address:
      - Institute for Geoinformatics
      - Heisenbergstr. 2
      - 48149 Münster, Germany
      - \href{https://orcid.org/0000-0002-0024-5046}{\includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} https://orcid.org/0000-0002-0024-5046}
    email: daniel.nuest@uni-muenster.de
  - name: Carl Boettiger
    #affiliation: -
    #email: -
  - name: Robrecht Cannoodt
    #affiliatin: -
    #email: -
  - name: Dirk Eddelbuettel
    affiliation: University of Illinois at Urbana-Champaign
    address:
      - Department of Statistics
      - Illini Hall, 725 S Wright St
      - Champaign, IL 61820, USA
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0001-6419-907X}{https://orcid.org/0000-0001-6419-907X}
    email: dirkd@eddelbuettel.com
  - name: Mark Edmondson
    affiliation: IIH Nordic A/S, Google Developer Expert for GCP
    email: mark@markedmondson.me
  - name: Colin Fay
    affiliation: ThinkR
    address:
      - 5O rue Arthur Rimbaud
      - 93300 Aubervilliers, France
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0001-7343-1846}{https://orcid.org/0000-0001-7343-1846}
    email: contact@colinfay.me
  - name: Ben Marwick
    #affiliation: -
    #email: -
  - name: Karthik Ram
    affiliation: Berkeley Institute for Data Science
    address:
      - University of California
      - Berkeley, CA 94720, USA
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0002-0233-1757}{https://orcid.org/0000-0002-0233-1757}
    email: karthik.ram@berkeley.edu
  - name: Noam Ross
    affiliation: EcoHealth Alliance
    address:
      - 460 W 34th St., Ste. 1701
      - New York, NY 10001, USA
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0002-0233-1757}{https://orcid.org/0000-0002-0233-1757}
    email: ross@ecohealthalliance.org
  - name: Nan Xiao
    affiliation: Seven Bridges Genomics
    address:
      - 529 Main St, Suite 6610
      - Charlestown, MA 02129, USA
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0002-0250-5673}{https://orcid.org/0000-0002-0250-5673}
    email: me@nanx.me
  - name: Lori Shepherd
    affiliation: Roswell Park Comprehensive Cancer Center
    address:
      - Elm & Carlton Streets
      - Buffalo, New York, 14263, USA
      - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0002-5910-4010}{https://orcid.org/0000-0002-5910-4010}
    email: lori.shepherd@roswellpark.org
  - name: Nitesh Turaga
    affiliation: Roswell Park Comprehensive Cancer Center
    address:
    - Elm & Carlton Streets
    - Buffalo, New York, 14263, USA
    - \includegraphics[height=\fontcharht\font`\0]{ORCID-iD_icon-vector.pdf} \href{https://orcid.org/0000-0002-0224-9817}{https://orcid.org/0000-0002-0224-9817}
    email: nitesh.turaga@roswellpark.org
abstract: >
  The Rocker project provides widely-used Docker images for R across different application scenarios.
  This articles surveys describe downstream projects building upon Rocker.
  We also look beyond Rocker to other projects connecting containerisation with R.
  These use cases and the diversity of applications demonstrate the power of Rocker and containerisation for collaboration, effectivity, scalability, and transparency.
# blank footnote? https://tex.stackexchange.com/questions/170511/footnotes-without-numbering
output:
  rticles::rjournal_article:
    keep_tex: true
---

# Introduction

The R community keeps growing: the number of new packages on CRAN keeps on rising, meetups, conferences, online courses and unconferences prosper, and the uptake in education and industry increases [REFs].
All this cements the role of R as the lingua franca of statistics, data visualisation, and computational research.
Coinciding with the rise of R was the advent of Docker [CITATION?] as a general tool for development, distribution, and deployment of software.
Combining both these topics, the _Rocker Project_ ([https://www.rocker-project.org/](https://www.rocker-project.org/)) was introduced in 2014 and provides a number of [Docker](https://en.wikipedia.org/wiki/Docker_(software)) images for various use cases as described in \citet{RJ-2017-065}.
The considerable uptake and continued evolution of the Rocker suite of containers has lead to numerous projects extending or building upon Rocker images ranging from reproducible research to production deployments.
This article presents this _Rockerverse_ of projects.
It also introduces related activities connecting the R language and environment with other containerisation solutions.
The main contributions is a coherent picture of the current lay of the land of containers in, with, and for `R`.
This diversity includes demonstrators, early prototypes, and mature projects.

# Containerization and Rocker [NN]

<!--_Do we need a generic intro here?_ DE: Yes, and happy to take a stab.-->

Docker, an application and service provide by the eponymous company, has in just a few short years risen to prominence for development, testing, deployment and distribution of computer software.  While there are related approaches such as LXC or Singularity, Docker has become synomymous with "containerization"---the method of taking software artefacts and bundling them in such a way that use becomes standardized and portable across operationg systems.   In doing so, Docker had recognised and validated the importance of one very important thread that had been emerging, namely virtualization. By allowing multiple concurrent applications or services to run concurrently on one host machine without any fear of interference between them, an important scalability feature had been realized.  But Docker improved on virtualization by accessing the host system---generally Linux---through a much thinner and smaller shim than a full operating system emulation or virtualization.  This makes for more efficient use of system resources and allowed another order of magnitude in terms of scalability of deployment. [Reference?]   While Docker makes use of Linux kernel features, its importance was large enough so that some required aspects of running Docker have been added to other operating systems to support Docker there more efficiently too. [Win10; reference?] 

The key accomplishment of Docker is to make a "bundled" aggregation of software available to any system running Docker, without requiring much else from the host.  This is a rather attractive, and novel, proposition which has lead to widespread adoption and use of Docker in a variety of domains.  It provided also a natural match for "cloud deployment" which runs (or at least appears to run) "seamlessly" without much explicit reference to the underlying machine, architecture or operating system.  Using containers on cloud hosted servers is a natural fit also because these containers can be deployed with very littler in terms of dependencies on the host system.

For statistical computing and analysis centered around R, the Rocker Project has provided a variety of Docker containers since its start in 2014 \citep{RJ-2017-065}. The Rocker Project provides several lines of containeres spanning to from building blocks with R-release or R-devel to containers with RStudio Server and Shiny Server. Also of note is a series of "versioned" containers which match the R release they contain with the _then-current_ set of packages via the MRAN Snapshot views of CRAN. [Maybe at a later point add forward reference to other topics building on Rocker described here.]

# Container Images

## Images for alternative R distributions

As outlined above, R is a widely-used language with a large community.
The large number of extension packages provides access to an unrivaled variety of established and upcoming features.
Nevertheless, special use cases and experimental projects exist to test approaches or provide features different to what _"base R"_ provides.
These projects stem both form academia and industry.
_Base R_, sometimes called `GNU-R`, is the R distribution maintained by the [R Core Team](https://www.r-project.org/contributors.html) and and provided via [CRAN](https://cran.r-project.org/).

**Microsoft R Open** ([MRO](https://github.com/nuest/mro-docker)) is an R distribution formerly known as Revolution R Open (RRO) before Revolution Analytics was acquired by Microsoft.
MRO is compatible with main R and it's packages.sss
"It includes additional capabilities for improved performance, reproducibility, and platform support." \citep{microsoft_mro}
Most notably these capabilities are the MRAN repository a.k.a. CRAN Time Machine, which is enabled by default, and the (optional) integration with [Intel® Math Kernel Library](https://software.intel.com/en-us/mkl) (MKL) for multi-threading in linear algebra operations \citep{microsoft_multithread}.
<!-- MRAN is also used by versioned Rocker images. -->
MRO does not provide official Docker images, but a set of community-maintained `Dockerfile`s and Docker images `nuest/mro` are available on Docker Hub at [https://github.com/nuest/mro-docker](https://github.com/nuest/mro-docker) and on GitHub at [https://github.com/nuest/mro-docker](https://github.com/nuest/mro-docker) respectively.
The images are inspired by the Rocker images and can be used much in the same fashion, effectively a drop-in replacement allowing users to quickly evaluate if the benefits of MRO + Intel® MKL apply to their use case.
Version-tagged images are provided for the latest bugfix release of recent R versions.
Extended license information about MKL is printed at every startup.

**Renjin** is an interpreter for the R language running on top of the [Java Virtual Machine](https://en.wikipedia.org/wiki/Java_virtual_machine) (JVM) providing full two-way access between Java and R code \citep{wikipedia_renjin_2018}.
It was developed to combine the benefits of R, such scripting and extension packages, with the JVM's advantages in the areas of security, cross-platform availability, and established position in enterprise settings.
R extension packages need to be specially compiled and are distributed via the Java package manager [Apache Maven](https://en.wikipedia.org/wiki/Apache_Maven), cf. [http://packages.renjin.org/packages](http://packages.renjin.org/packages) for available packages.
Packages are loaded on demand, i.e. at the first call to `library()`.
Not all R packages, especially one linking to binary libraries, are available, e.g. `rgdal` \footnote{\href{http://packages.renjin.org/package/org.renjin.cran/rgdal/1.4-4/build/1}{http://packages.renjin.org/package/org.renjin.cran/rgdal/1.4-4/build/1}}.
There are no offical Docker images for Renjin, but community-maintained images for selected releases only are available under `nuest/renjin` on Docker Hub and GitHub at [https://hub.docker.com/r/nuest/renjin](https://hub.docker.com/r/nuest/renjin) and [https://github.com/nuest/renjin-docker](https://github.com/nuest/renjin-docker) respectively.
These images expose the command line interface of Renjin in a similar fashion as Rocker images and allow an easy evaluation of Renjin's suitability, but are not intended for production use.

**[pqR](http://www.pqr-project.org/)** is a "a pretty quick version of R".
_pqR_ fixes some opinionated issues in the R language and is the basis for experimental features, e.g. automatic diffentiation\footnote{ \href{https://riotworkshop.github.io/abstracts/riot-2019-pqr.txt}{https://riotworkshop.github.io/abstracts/riot-2019-pqr.txt}}.
The source code development [on GitHub](https://github.com/radfordneal/pqR/) is a one man project and it does not provide any Docker images.
But especially disruptive approaches may contribute to the development of the R ecosystem, so the `nuest/pqr` images on Docker Hub and GitHub at [https://hub.docker.com/r/nuest/pqr/](https://hub.docker.com/r/nuest/pqr/) and [https://github.com/nuest/pqr-docker](https://github.com/nuest/pqr-docker) respectively.

**FastR** is "A high-performance implementation of the R programming language, built on GraalVM" (https://github.com/oracle/fastr).
It is developed by Oracle, connects R to the GraalVM ecosystem \citep{wikipedia_graalvm_2019}, and also claims superior performance but also targets full compatibility with base R \footnote{ \href{https://github.com/oracle/fastr}{https://github.com/oracle/fastr}}.
There are no offical Docker images provided, but `nuest/fastr` images and `Dockerfile` are independently maintained on [Docker Hub](https://cloud.docker.com/repository/docker/nuest/fastr/) and [GitHub](https://github.com/nuest/fastr-docker) respectively.

While the images presented in this section are far from being as vetted, stable, and widely used as any of the Rocker images, they demonstrate an important advantage of containerisation technology, namely the abiliy to transparently build portable stacks of open source software and make them easily accessible to users.
All different distributions are published under GPL licenses.
Since all of the different R distributions claim better performance as a core motivation, a comparision based on Docker images, potentially leveraging the [resource restriction mechanisms](https://docs.docker.com/config/containers/resource_constraints/) of Docker to level the playing field, seems useful future work.

## Bioconductor

[_Bioconductor_](http://bioconductor.org/) is an open source, open development project for the analysis and comprehension of genomic data \citep{gentleman_bioconductor_2004}.
The project consists of 1741 R software packages as of August 15th 2019, as well as packages containing annotation or experiment data.
_Bioconductor_has a semiannual release cycle, each release is associated with a particular version of R.
Docker images allow availability of current and past versions of _Bioconductor_ for convenience and reproducibility.
_Bioconductor_ 'base' docker images are built on top of `rocker/r-ver` and `rocker/rstudio`.
_Bioconductor_ installs packages based on the R version, and therefore uses `rocker/rstudio` and `rocker/r-ver` version tagging.
_Bioconductor_ selects the desired version of R from Rocker, adds the BiocManager CRAN package for installing appropriate versions of _Bioconductor_ packages, and creates a _Bioconductor_ docker image with an informative tag (R\_version\_Bioc\_version).
The images are summarized on the _Bioconductor_ web site (https://bioconductor.org/help/docker/), maintained on GitHub ([https://github.com/Bioconductor/bioc_docker](https://github.com/Bioconductor/bioc_docker)), and available to the community through [DockerHub](https://hub.docker.com/).
Past and current combinations of R and _Bioconductor_ are therefore accessible via a specific docker tag.

_Bioconductor_ has several images in addition to 'base', specific to various areas of research.
The 'core' image installs the most commonly used _Bioconductor_ packages.
_Bioconductor_ images for proteomics, metabolomics, and flow cytometry are community maintained.
All community maintained images build on top of the _Bioconductor_ base image and therefore indirectly the Rocker images.
To simplify building and maintaining _Bioconductor_ images, we use a Ruby templating engine.
A recent audit of the _Bioconductor_ Dockerfiles, following best practices from the Docker website, led to a reduction in the size and number of layers.
The most important insights involve the ['union' file system used by Docker](https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-container-reads-and-writes-work-with-overlay-or-overlay2).
In this file system, once a layer (e.g., `RUN` statement) writes to a file path, the file path is never altered.
A subsequent layer that might appear to remove or overwrite the path actually masks, rather than alters, the original.
It is therefore important to clean up (e.g., cache removal) within each layer, and to avoid re-installing existing dependencies.

A recent innovation is to produce a `bioconductor_full` image to emulate the _Bioconductor_ nightly Linux build machine.
The image contains the _system dependencies_ needed to install and check almost all (1730 of 1741) _Bioconductor_ software packages.
Users no longer have to manage complciated system dependencies.
The image is configured so that `.libPaths()` has `/usr/local/lib/R/host-site-library` as the first location.
Users mounting a location on the host file system to this location then persist installed packages across docker sessions or updates.
Many _R_ users pursue flexible work flows tailored to particular analysis needs, rather than standardized work flows.
The `bioconductor_full` image is well-suited to this pattern.
`bioconductor_full` provides developers with a test environment like _Bioconductor_'s build system.

Use of images suggests several interesting possibilities for the _Bioconductor_ project.
Images may be valuable in teaching, where participants pull pre-built images to avoid complicated configuration of their own computing environemnts.
An appeal of this over our current approach (providing Amazon Machine Instances for the duration of the course) is the utility of the image to participant after the course is over.
`bioconductor_full` introduces a common system configuration, so it becomes increasingly sensible for _Bioconductor_ to distribute convenient _binary_ packages.
Images also suggest approaches to more advanced computational models.
For instance, we are exploring use of images for [Helm](https://helm.sh/)-orchestrated [Kubernetes](https://kubernetes.io/) clusters on the Google Cloud Platform.
The user interacts with a manager image based on `bioconductor_full`, configured to perform map-reduce style computations via the BiocParallel package communicating with minimally-configured worker images.
A strength of this approach is that the responsibility for complex software configuration (including customized development) is shifted from the user to the experienced _Bioconductor_ core team.

## Images for (historic) R versions [@ColinFay, @nuest]

As with any other software, each new version of R comes with its share of changes. 
Some are breaking changes, some are not, but the fact is that running a piece of code in a given version might give different s from another version ; which, in some case, can be an issue when it comes to reproducibility.
For example, think about R random seed: starting with `R 3.6.0`, running `set.seed(2811); sample(1:1000, 2)` will not give the same result as if it was run inside an older version of R.
That might seem trivial, but all the code using the random number generator will not be exactly reproducible after this breaking change. 
Detecting this kind of behavior is the reason why the [r-online](https://github.com/ColinFay/ronline) app has been developped: available as a standalone NodeJS app or [online](https://srv.colinfay.me/r-online), this app was designed for historic exploration of R: with it, the user can compare a piece of code run in two separate versions of R. 
Internally, this app takes the piece of code written by the user, opens one or two docker instances with the given version of R, and returns the result to the user. 

Containers are perfectly suited to capture a specific configuration of a computing environment to prohibit such problems.
The Rocker images provides the _versioned stack_ for different the version of R since the project inception, which use the _then-current_ stable Debian base image \citep[cf.][]{RJ-2017-065}.
Based on custom build phase hooks\footnote{\href{https://docs.docker.com/docker-hub/builds/advanced/}{https://docs.docker.com/docker-hub/builds/advanced/}}, i.e. small shell scripts executed at different phases in the automated build of Rocker images, there are also semantic version tags for the most recent freezed (i.e. using MRAN) versions with only the major and minor version\footnote{See \href{https://github.com/rocker-org/rocker-versioned/blob/master/VERSIONS.md}{https://github.com/rocker-org/rocker-versioned/blob/master/VERSIONS.md}, FIXME: \href{https://github.com/rocker-org/rocker-versioned/issues/42}{https://github.com/rocker-org/rocker-versioned/issues/42}.}.
So, at the time of writing this article, `rocker/r-ver:3` and `3.6` are aliases for `3.6.0`, because `3.6.1` is the latest release.
With the release of `3.6.2` and pinning of the MRAN version in `3.6.1`, the tags `3` and `3.6` will deliver the same image as `3.6.1`.
These tags allow users to update the base image to retrieve bugfixes while reducing the risk of also introducing breaking changes.

## Windows Images [@nuest]

- [rocker-win](https://github.com/nuest/rocker-win)
- is possible, only relevant in organisations with an existing Windows Server-based infrastructure, can meet policies then

## Non-Debian Linux images [NN]

- Alpine images
- [Images used by R-Hub](https://github.com/r-hub/rhub-linux-builders) (overlap with CI?)
- https://github.com/jlisic/R-docker-centos

# R Packages
<!-- from packages closer to Docker to second level packages to more complex ones -->

## Clients

A number first order R packages exist that talk directly to Docker.
They can be used to control the Docker daemon, e.g. to build images or run containers, from R without explicit knowledge of the Docker command line interface.

- [`stevedore`](https://github.com/richfitz/stevedore)
- [`docker`](https://bhaskarvk.github.io/docker/)
- [`harbor`](https://github.com/wch/harbor/)
- [`dockermachine`](https://github.com/cboettig/dockermachine)
- [`dockyard`](https://github.com/thebioengineer/dockyard)

## Capture and create environments [@nuest]

Several second order R packages attempt to make the process of creating Docker images and using containers for specific tasks, such as running tests or rendering reproducible reports, easier.

- [`dockerfiler`](https://github.com/ColinFay/dockerfiler/)
- [`containerit`](https://github.com/o2r-project/containerit/)
- [`dockertest`](https://github.com/traitecoevo/dockertest/)

**[_liftr_](https://nanx.me/liftr/)** \citep{liftr2019} aims to solve the problem of persistent reproducible reporting in statistical computing.
Currently, the R Markdown format and its backend compilation engine _knitr_ offer a _de facto_ standard for creating dynamic documents \citep{xie2018}.
However, the reproducibility of such content authoring environments is often limited to individual machines --- it is not easy to replicate the system environment (libraries, R versions, R packages) where the document was compiled.
This issue becomes even more serious when it comes to collaborative document authoring and creating large-scale document building services.
_liftr_ solves this reproducibility problem by bringing Docker to the game.
In essence, _liftr_ helps R Markdown users create and manage Docker containers for rendering the documents, thus make the computations utterly reproducible across machines and systems.

On implementation, with no side effects, _liftr_ extended and introduced new metadata fields to R Markdown, allowing users to declare the dependencies for rendering the document.
_liftr_ parses such fields and generates a `Dockerfile` for creating Docker containers.
_liftr_ then helps render the document inside the created Docker container.
This workflow is summarized in Figure \ref{figure:liftr}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{liftr-workflow}
  \caption{The liftr workflow for rendering containerized R Markdown documents.}
  \label{figure:liftr}
\end{figure}

_knitr_ and R Markdown are used as the template engine to generate the `Dockerfile`.
Features such as caching container layers for saving image build time, automatic housekeeping for fault-tolerant builds, and Docker status check are supported by _liftr_.
Four RStudio addins are also offered by _liftr_ to allow push-button compilation of documents and provide better IDE integrations.

Three basic principles are followed to design the _liftr_ package since its inception.

1. Continuous reproducibility.
Continuous integration, continuous delivery, and continuous deployment are well-accepted practices in software engineering.
Similarly, it is believed by the authors of _liftr_ that ensuring computational reproducibility means a continuous process instead of creating static data/code archives or a one-time deal.
Specifically, the software packages used in data analysis should be upgraded regularly in a manageable way.
Therefore, _liftr_ supports specifying particular versions of package dependencies, while users are encouraged to always use the latest version of packages (without a version number) by default.

2. Document first.
Many data analysis workflows could be wrapped as either R packages or dynamic documents.
In _liftr_, the endpoint of dynamic report creation is the focus of containerization, because this offers more possibilities for organizing both computations and documentation.
Users are encouraged to start thinking from the visible research output from the first day.

3. Minimal footprint.
R Markdown and Docker are already complex software systems.
Making them work together seamlessly can be complicated.
Therefore, API designs such as function arguments are simplified while being kept as expressive and flexible as possible.

In summary, _liftr_ tries to redefine the meaning of computational reproducibility by offering system-level reproducibility for data analysis.
It provided a practical way for achieving it --- a new perspective on how reproducible research could be done in reality.
Further, sharing system environments for data analysis also becomes extremely easy, since users only need to share the R Markdown document (with a few extra metadata fields), and compile them with _liftr_.
As an example, _liftr_ demonstrated its advantage for R Markdown-based computational workflow orchestration, by effortlessly containerizing 18 complex _Bioconductor_ workflows in the DockFlow project (https://dockflow.org) in 2017.

- [`rize` for Shiny](https://github.com/cole-brokamp/rize)

One of the key tool of Docker are dockerfiles, which can be thought of as "recipes" used to build a specific image. 
But building these files can feel like a workflow break as it demands to open a separate file, and / or to write a small wrapper around R's `write()` function to add elements to the file. 
At the same time, doing it by hand prevents from using programming language for what they are good for: iteration, and automation. 
Iteration and automation for Dockerfiles is the very reason behind [`dockerfiler`](https://github.com/ColinFay/dockerfiler), an R package designed for building Dockerfiles straight from R. 

For example, the **[`golem`](https://github.com/ThinkR-open/golem)** package makes an heavy use of `dockerfiler` when it comes to creating the `Dockerfile` for building production-grade Shiny applications and deploying them.
The reason behind this is that `dockerfiler`, on top of being scriptable from R, can leverage all the tools available in R to parse a `DESCRIPTION` file, to get system requirements, to list dependencies, versions, etc. 

# Use cases and applications

## Deployment to cloud services [div]

The cloud is the natural environment of containers, and becomes the go-to mechanism to expose applications written in R to users.

- [`babelwhale`](https://cran.r-project.org/web/packages/babelwhale/index.html): Running a Docker from R with Singularity or Docker as back-end. This is really useful in HPC environments where a user might not have root access but is able to install Singularity instead of Docker. [@rcannood]
- `RSelenium`
- [`googleComputeEngineR`](https://cloudyr.github.io/googleComputeEngineR/) (function `gce_vm_template()`)
  - To enable quick deployments of key R services such as RStudio and Shiny onto cloud virtual machines (VMs), this package utilises Dockerfiles to move the labour of setting up those services from the user to a premade Docker image.  For example, by specifying the template `template="rstudio"` in `gce_vm_template()/gce_vm()` an up to date RStudio Service image is launched.  Specifying `template="rstudio-gpu"` will launch an RStudio Server image with a GPU attached, etc.  
- [`analogsea`](https://github.com/sckott/analogsea) (digital ocean R client)
- [`plumber`](https://www.rplumber.io/docs/hosting.html#docker)
- [production deployment of neural networks](https://github.com/tmobile/r-tensorflow-api) [@jnolis, @nolistic]

## Continuous integration and continuous delivery [@noamross, @ColinFay]

The controlled nature of containers, i.e. it is possible to define the software environment very well, even on remote machines, make them also useful for continuous integration (CI) and continuous delivery of applications.

- DevOps
  - https://www.opencpu.org/posts/opencpu-with-docker/

When doing continuous integration and continuous delivery, it's crucial to test in an environment that matches the production environment.
Tools like `Gitlab-CI` are built on top of Docker images: the user specifies a base Docker image, and the whole tests are run inside this environment. 
But, as we just said, this environment has to be fixed, but it also have to contain the necessary toolkit. 
In order to achieve that, [`r-ci`](https://github.com/ColinFay/r-ci) combines `rocker` versioning and a series of tools specifically designed for testing. 
That way, package builders can use this image as a base image for there testing environment, without having to install the necessary packages every time they need to run a new test.

- [dynwrap](https://github.com/dynverse/dynwrap_containers/blob/master/.travis.yml) [@rcannood]
  - For this project, we use travis-ci to build rocker-derived containers, test them, and only push them to docker hub (from travis-ci.org) if the integration tests succeed.
- Google Cloud Build
  - https://cloud.google.com/cloud-build/
  - Cloud Build is another continuous intergation service that helps move the workload of building the Docker images to an online service.  Cloud Build can be set up to build the Dockerfiles on each GitHub commit or release.  This means you do not need to build the Docker images locally, which can tie up resources since Docker images can be several GBs and take a long time to compile. Google Cloud Build works alongside Google Container Registry to allow you to build private and public Docker images, which allows you to build up your own dependency graphs for downstream applications.

## Common or public work environments

The [Binder project](https://mybinder.readthedocs.io/en/latest/), maintained by the team behind Jupyter, makes it possible for users to create and share their computing environments with others.
A deployed version of this service called binder hub allows anyone with access to a web browser and an internet connection to launch a temporary instance of these custom environments and execute any notebooks contained within.
From a reproducibility standpoint, Binder makes it exceedingly easy to compile a paper, visualize data, and run small examples from papers or tutorials without the need for any local installation. 

To set up Binder for a project, a user typically starts at an instance of a Binderhub (examples include mybinder.org and binder.pangeo.io) and passes the location of a hosted Git repository.
Using internal tools such as [repo2docker](https://repo2docker.readthedocs.io/en/latest/config_files.html), the binderhub builds a Dockerfile by parsing the contents of all code contained within.
While this approach works well for most run of the mill Python projects, it is not so seamless for R projects.
For any R projects that use the Tidyverse suite [citation in review. Will add by the time this gets submitted], the time and resources required to build all dependencies from source can often time out before completion, making it frustrating for the average R user. 

[`holepunch`](https://github.com/karthik/holepunch) is a R package that was designed to remove some of these limitations and make Binder more accessible to novice R users.
`holepunch` achieves this by leveraging Rocker images that contains the Tidyverse along special Jupyter dependencies, and only installs additional packages from CRAN and Bioconductor that are not already part of these images.
Starting with the Binder/Tidyverse base images eliminates a large part of the build time and in most cases results in a binder instance launching within a minute. `holepunch` as a side effect also creates a DESCRIPTION file which then turns any project into a research compendium [doi:10.1080/00031305.2017.1375986].
The Dockerfile included with the project can also be used to launch a RStudio server locally independent of binder which is especially useful when more computational resources are required.

One use for containers is to run on shared local hardware where teams manage their own high-performance servers.
This can follow one of several design patterns: users may deploy containers to hardware as a work environment for a specific project, conatiners may provide per-user persistent environments, or a single container can act as a common multi-user environment for a server.
The former models provide modularity, while the latter approach is most similar to a simple shared server.
In all cases, though, the containerized approach provides several advantages: First, users may use the same image and thus work environment on desktop and laptop computers, as well.
Second, software updates can be achieved by updating and redeploying the container, rather than tracking local installs on each server.
Third, the containerized environment can be quickly deployed to other hardware, cloud or local, if more resources are neccessary or in case of server destruction or failure.
In any of these cases, users need a method to interact with the containers, be it and IDE, or command-like access and tools such as SSH, which is usually not part of standard container recipes and must be added.
The Rocker project provides containers pre-installed with the RStudio IDE.

In cases where users store nontrivial amounts of data for their projects, data needs to persist beyond the life of the container.
This may be via in shared disks, attached network volumes, or in separate storage where it is uploaded between sessions.
In the case of shared disks or network-attached volumes, care must be taken to persist user permissions, and of course backups are still neccessary.
When working with multiple servers, an automation framework such as [Ansible](https://www.ansible.com) may be useful for managing users, permisions, and disks along with containers.

In some cases, containers are not completely portable between hardware environments.
This is the case for software using graphical processing units (GPUs) which are increasingly popular for compute-intensive machine learning tasks.
Containers running GPU software  require drivers and libraries specific to GPU models and versions, and containers require a specialized runtime to connect to the underlying GPU hardware.
For NVIDIA GPUs, the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker) includes a specialized runtime plugn for docker and set of base images with appropriate drivers and libraries, and the Rocker project has (beta) images based on these that include GPU-enabled versions of machine-learning R packages.

R is a fantastic tool when it comes to interfacing with databases: almost every open source and proprietary database system has an R package that allows users to connect and interact with the it 
This flexibility is even broader now that we have tools like `DBI`, that allows to create a common API for interfacing these databases, or like `dbplyr`, which are designed to run `dplyr` code straight against the database. 
But learning and teaching these tools comes with a cost: the cost of deploying or having access to an environment with the softwares and drivers installed. 
For people teaching R, it can become a barrier if they need to install local versions of the drivers, or to connect to remote instances which might or might not be made available by the IT. 
Giving access to a sandbox for the most common database environments is the idea behind [`r-db`](https://github.com/ColinFay/r-db) [@ColinFay], a docker image that contains everything needed to connect, from R, to a database. 
Notably, with `r-db`, the users don't have to install complex drivers or to configure their machine in a specific way. 
On top of that, `r-db` comes with a comprehensive [online guide](http://colinfay.me/r-db/), explaining how to spin a docker instance of a specific database, and how to use `r-db` to connect to it. 
Each package contained inside this image also has a series examples, so that the user can get started with the database right away.

- [RCloud](https://github.com/att/rcloud/tree/master/docker) uses a `rocker/drd` base image for creation of collaborative data analysis and visualisation environments.
- [ShinyProxy](https://www.shinyproxy.io/) [creates a container](https://github.com/openanalytics/shinyproxy/blob/master/src/main/java/eu/openanalytics/services/DockerService.java#L388) for each user

## Processing [div]

The portability of containers becomes particularly useful when complex processing tasks shall be offloaded to a server.

- Docker images for cloud services
  - A popular application of using Docker is to create the environments that can run in parallel for speeding up R code.  For example, [`googleComputeEngineR`](https://CRAN.R-project.org/package=googleComputeEngineR)'s `gce_vm_cluster()` function can create clusters of 2 or more virtual machines, running multi-CPU architectures.  Instead of running a local R script with the local CPU and RAM restrictions, the same code can be processed on all CPU threads of the cluster of machines in the cloud, all running in a Docker container with the same R environments.   This is achieved through `googleComputeEngineR`'s integration with the R parralisation library [`library(future)` by Henrik Bengtsson]( https://CRAN.R-project.org/package=future).  Local R computation can be thrown up to a multi-CPU and VM environment to achieve parrell computation in a few lines of R code in your local session. - [some demonstrations are available here](https://cloudyr.github.io/googleComputeEngineR/articles/massive-parallel.html). 
- Google Cloud Run - This is a CaaS (Containers as a Service) that lets you launch a Docker container without worrying about underlying infrastructure.  This dispenses with the developer creating a cloud server to run the Docker image on, by abstracting away those servers to a more serverless configuration.  Cloud Run lets you run your code on top of a managed or your own Kubernetes cluster running Knative, and can accept any Docker image.  The service takes care of network ingress, scaling machines up and down to zero, authentication and authorisation, all features which are non-trivial for a developer to create on their own.  This can be used to scale up R code to millions of instances if they need to, with little or no changes to existing code. An R implementation is shown here at [cloudRunR](https://github.com/MarkEdmondson1234/cloudRunR) which uses Cloud Run to create a scalable R plumber API. 
- `batchtools` \citep{Lang2017batchtools} can [schedule jobs with Docker Swarm](https://mllg.github.io/batchtools/reference/makeClusterFunctionsDocker.html)
- scalable deployments, e.g. start with numerous Shiny talks mentioning Rocker at useR!2017
- [dynmethods](https://github.com/dynverse/dynmethods) [@rcannood]: In order to evaluate ±50 computational methods which all used different environments (R, Python, C++, ...), we wrapped each of them in a docker container and can execute these methods from R. Again, all of these containers are being built on travis-ci, and will only be pushed to docker hub if the integration test succeeds.

## Research Compendia [@benmarwick]

...

## Development and debugging

Containers can also serve as useful playgrounds to create environments ad-hoc or to provide very specific environments that are not often needed.
Developers can readily start a local container based on a Rocker image to investigate a bug report, which might require them to install a specific combination of package versions or use a specific R version (see also versioned images above).
Unlike recreating the reporter's environment on their regular machine, a container can simply be discarded after use, and there is no need to tediously roll back explorative changes to a previous state.
Using the Rocker images with RStudio, the disposable environments lack no development comfort (see also the usage in research compendia).

**[`r-debug`](https://github.com/wch/r-debug)** is a purpose built Docker image for debugging R memory problems.
\citet{eddelbuettel_debugging_2019} describes how a Docker container was used to debug an issue with a package only occuring with a particular version of Fortran, and using tools which are not readily available on all platforms (e.g., not on macOS).

- **[R-Hub](https://github.com/r-hub/)**

# Other containerisation platforms

Docker is not the only containerisation software.
An alternative stemming from the domain of high-perfomance computing is Singularity \citep{kurtzer_singularity_2017}.
Singularity can run Docker images, and in the case of Rocker works out of the box if the main process is R, e.g., in `rocker/r-base`, but does not succeed in running images where there is an init script, e.g. in containers that by default run RStudio.
In the latter case, a `Singularity` file, a set of instructions akin to a `Dockerfile` needs to be used.

# Discussion/outlook/conclusion [@all, please add bullet points]

- next steps
  - consolidation?
- support more complex set-ups with ready-to-use stacks, e.g. GPU, AI, ML
- Missing pieces?
- consolidation (e.g. via packages using `dockerfiler` and `stevedore`)
- Common themes
  - reproducibility
- will knowledge about containers continue to spread?
- what is needed for even more containers with R?
- the ability to move processing between services easily (e.g locally, one cloud providers VM, another cloud provider's Container-as-a-Service)
- running RStudio in a container is a common approach for reproducibility - _is there a need to run/control containers from a UI integrated in RStudio?_
- moving away from Docker focus towards containers?
  - is there a need for a wrapper for [podman's Varlink-API](https://github.com/containers/libpod/blob/master/API.md) (similar to stevedore) and buildah to manage containers and images respectively with alternative software stacks?
  - will there be control packages for Singularity support?
- ...

# Author contributions

DN conceived of the presented idea and \href{https://github.com/nuest/rockerverse-paper/issues/3}{initialised the formation of the writing team} and wrote the section about images for R distributions.
CB ..
RC ..
DE ..
ME ..
CF wrote the section about `r-online`, `dockerfiler`, `r-ci` and `r-db`.
BW ..
KR ..
NR ..
NX wrote the section on liftr.
LS \& NT wrote the section on Bioconductor.
All authors contributed to the discussion and outlook section and approved the final version.
This articles was collaboratively written at \href{https://github.com/nuest/rockerverse-paper/}{https://github.com/nuest/rockerverse-paper/}.
The \href{https://github.com/nuest/rockerverse-paper/graphs/contributors}{contributors page} and \href{https://github.com/nuest/rockerverse-paper/commits/master}{commit history} provide a detailed view on the respective contributions.

\bibliography{RJreferences}
