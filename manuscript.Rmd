---
title: "The Rockerverse: Images, Packages, and Applications for Containerization with R"
author:
  - name: Daniel Nüst
    # https://github.com/nuest
    affiliation: University of Münster
    address:
      - Institute for Geoinformatics
      - Heisenbergstr. 2
      - 48149 Münster, Germany
      - \orcid{0000-0002-0024-5046}
    email: daniel.nuest@uni-muenster.de
  - name: Dirk Eddelbuettel
    # https://github.com/eddelbuettel
    affiliation: University of Illinois at Urbana-Champaign
    address:
      - Department of Statistics
      - Illini Hall, 725 S Wright St
      - Champaign, IL 61820, USA
      - \orcid{0000-0001-6419-907X}
    email: dirkd@eddelbuettel.com
  - name: Dom Bennett
    # https://github.com/DomBennett
    affiliation: Gothenburg Global Biodiversity Centre, Sweden
    address:
      - Carl Skottsbergs gata 22B
      - 413 19 Göteborg, Sweden
      - \orcid{0000-0003-2722-1359}
    email: dominic.john.bennett@gmail.com
  - name: Robrecht Cannoodt
    # https://github.com/rcannood
    affiliation: Ghent University
    address:
      - Data Mining and Modelling for Biomedicine group
      - VIB Center for Inflammation Research
      - Technologiepark 71
      - 9052 Ghent, Belgium
      - \orcid{0000-0003-3641-729X}
    email: robrecht@cannoodt.dev
  - name: Dav Clark
    # https://github.com/davclark
    affiliation: Gigantum, Inc.
    address:
      - 1140 3rd Street NE
      - Washington, D.C. 20002, USA
      - \orcid{0000-0002-3982-4416}
    email: dav@gigantum.com
  - name: Gergely Daroczi
    # https://github.com/daroczig
    address:
      - \orcid{0000-0003-3149-8537}
    email: daroczig@rapporter.net
  - name: Mark Edmondson
    affiliation: IIH Nordic A/S, Google Developer Expert for GCP
    email: mark@markedmondson.me
  - name: Colin Fay
    # https://github.com/ColinFay
    affiliation: ThinkR
    address:
      - 5O rue Arthur Rimbaud
      - 93300 Aubervilliers, France
      - \orcid{0000-0001-7343-1846}
    email: contact@colinfay.me
  - name: Ellis Hughes
    # https://github.com/thebioengineer
    affiliation: Fred Hutchinson Cancer Research Center 
    address:
      - Vaccine and Infectious Disease
      - 1100 Fairview Ave. N., P.O. Box 19024
      - Seattle, WA 98109-1024, USA
      #- \orcid{}
    email: ehhughes@fredhutch.org
  - name: Sean Lopp
    # https://github.com/slopp
    affiliation: RStudio, Inc
    address:
      - 250 Northern Ave
      - Boston, MA 02210, USA
      #- \orcid{}
    email: sean@rstudio.com
  - name: Ben Marwick
    # https://github.com/benmarwick/
    affiliation: University of Washington
    address:
      - Department of Anthropology
      - Denny Hall 230, Spokane Ln
      - Seattle, WA 98105, USA
      - \orcid{0000-0001-7879-4531}
    email: bmarwick@uw.edu
  - name: Heather Nolis
    # https://github.com/nolistic/
    affiliation: T-Mobile
    address:
      - 12920 Se 38th St.
      - Bellevue, WA, 98006, USA
      #- \orcid{NA}
    email: heather.wensler1@t-mobile.com
  - name: Jacqueline Nolis
    # https://github.com/jnolis/
    affiliation: Nolis, LLC
    address:
      - Seattle, WA, USA
      - \orcid{0000-0001-9354-6501}
    email: jacqueline@nolisllc.com
  - name: Hong Ooi
    # https://github.com/Hong-Revo
    affiliation: Microsoft
    address:
    - Level 5, 4 Freshwater Place
    - Southbank, VIC 3006, Australia
    email: hongooi@microsoft.com
  - name: Karthik Ram
    # https://github.com/karthik
    affiliation: Berkeley Institute for Data Science
    address:
      - University of California
      - Berkeley, CA 94720, USA
      - \orcid{0000-0002-0233-1757}
    email: karthik.ram@berkeley.edu
  - name: Noam Ross
    # https://github.com/noamross
    affiliation: EcoHealth Alliance
    address:
      - 460 W 34th St., Ste. 1701
      - New York, NY 10001, USA
      - \orcid{0000-0002-0233-1757}
    email: ross@ecohealthalliance.org
  - name: Lori Shepherd
    # https://github.com/lshep
    affiliation: Roswell Park Comprehensive Cancer Center
    address:
      - Elm & Carlton Streets
      - Buffalo, NY, 14263, USA
      - \orcid{0000-0002-5910-4010}
    email: lori.shepherd@roswellpark.org
  - name: Nitesh Turaga
    # https://github.com/nturaga
    affiliation: Roswell Park Comprehensive Cancer Center
    address:
      - Elm & Carlton Streets
      - Buffalo, NY, 14263, USA
      - \orcid{0000-0002-0224-9817}
    email: nitesh.turaga@roswellpark.org
  - name: Craig Willis
    # https://github.com/craig-willis
    affiliation: University of Illinois at Urbana-Champaign
    address:
      - 501 E. Daniel St.
      - Champaign, IL 61820, USA
      - \orcid{0000-0002-6148-7196}
    email: willis8@illinois.edu
  - name: Nan Xiao
    # https://github.com/nanxstats
    affiliation: Seven Bridges Genomics
    address:
      - 529 Main St, Suite 6610
      - Charlestown, MA 02129, USA
      - \orcid{0000-0002-0250-5673}
    email: me@nanx.me
  - name: Charlotte Van Petegem
    # https://github.com/charvp
    affiliation: Ghent University
    address:
      - Department WE02
      - Krijgslaan 281, S9
      - 9000 Gent, Belgium
      - \orcid{0000-0003-0779-4897}
    email: charlotte.vanpetegem@ugent.be
abstract: >
  The Rocker&nbsp;Project provides widely-used Docker images for R across different application scenarios.
  This articles surveys downstream projects building upon Rocker and presents the current state of R packages for managing Docker images and controlling containers.
  We also look beyond Rocker to other projects connecting containerisation with R, namely alternative suites of images.
  These use cases and the variety of applications demonstrate the power of Rocker specifically and containerisation in general.
  We identified common themes across this diversity: reproducible environments, scalability and efficiency, and portability across clouds.
# blank footnote? https://tex.stackexchange.com/questions/170511/footnotes-without-numbering
output:
  rticles::rjournal_article:
    keep_tex: true
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
library(tidyverse)
```

# Introduction
\label{intro}

The R community keeps growing.
This can be seen in the number of new packages on CRAN, which keeps on growing exponentially \citep{cran:2019}, but also in the numbers of conferences, open educational resources, meetups, unconferences, and companies taking up, as exemplified by the useR! conference series\footnote{\href{https://www.r-project.org/conferences/}{https://www.r-project.org/conferences/}}, the global growth of the R and R-Ladies user groups\footnote{\href{https://www.r-consortium.org/blog/2019/09/09/r-community-explorer-r-user-groups}{https://www.r-consortium.org/blog/2019/09/09/r-community-explorer-r-user-groups}, \href{https://www.r-consortium.org/blog/2019/08/12/r-community-explorer}{https://www.r-consortium.org/blog/2019/08/12/r-community-explorer}}, or the foundation and impact of the R&nbsp;Consortium\footnote{\href{https://www.r-consortium.org/news/announcements}{https://www.r-consortium.org/news/announcements}, \href{https://www.r-consortium.org/blog/2019/11/14/data-driven-tracking-and-discovery-of-r-consortium-activities}{https://www.r-consortium.org/blog/2019/11/14/data-driven-tracking-and-discovery-of-r-consortium-activities}}.
All this cements the role of R as the _lingua&nbsp;franca_ of statistics, data visualisation, and computational research.
Coinciding with the rise of R was the advent of [Docker](https://en.wikipedia.org/wiki/Docker_(software)) as a general tool for distribution and deployment of server applications---in fact, Docker can be called the _lingua&nbsp;franca_ of describing computing environments and packaging software.
Combining both these topics, the _Rocker&nbsp;Project_ ([https://www.rocker-project.org/](https://www.rocker-project.org/)) provides images with R (see the next Section for more details).
The considerable uptake and continued evolution of the Rocker&nbsp;Project has lead to numerous projects extending or building upon Rocker images, ranging from reproducible research\footnote{"Reproducible" in the sense of the _Claerbout/Donoho/Peng_ terminology \citep{barba_terminologies_2018}.} to production deployments.
This article presents this _Rockerverse_ of projects across all development stages: early demonstrations, working prototypes, and mature products.
We also introduce related activities connecting the R language and environment with other containerisation solutions.
The main contributions is a coherent picture of the current lay of the land of using containers in, with, and for R.

The article continues with a brief introduction of containerization basics and the Rocker&nbsp;Project, followed by descriptions of projects developing images with R installations.
Then, we present applications, starting with the R packages specificly for interacting with Docker, second-level packages using containers indirectly or only for specific features, up to complex use cases leveraging containers.
We conclude with a reflection on the landscape of packages and applications and point out future directions of development.

# Containerization and Rocker
\label{containerisation}
\label{rocker}

Docker, an application and service provide by the eponymous company, has in just a few short years risen to prominence for development, testing, deployment and distribution of computer software \citep[cf.][]{datadog_8_2018,munoz_history_2019}.
While there are related approaches such as LXC\footnote{\href{https://en.wikipedia.org/wiki/LXC}{https://en.wikipedia.org/wiki/LXC}} or Singularity \citep{kurtzer_singularity_2017}, Docker has become synomymous with "containerization"---the method of taking software artefacts and bundling them in such a way that use becomes standardized and portable across operationg systems.
In doing so, Docker had recognised and validated the importance of one very important thread that had been emerging, namely virtualization.
By allowing (one or possibly) multiple applications or services to run concurrently on one host machine without any fear of interference between them, an important scalability opportunity is being provided.
But Docker improved this compartmentalization by accessing the host system---generally Linux---through a much thinner and smaller shim than a full operating system emulation or virtualization.
This containerization is also called operating-system-level virtualization \citep{wikipedia_contributors_os-level_2020}.
Typically a container runs one process, whereas virtualization may run whole operating systems at a larger footprint.
This makes for more efficient use of system resources \citep{felter_updated_2015} and allowed another order of magnitude in terms of scalability of deployment \citep[cf.][]{datadog_8_2018}.
While Docker makes use of Linux kernel features, its importance was large enough so that some required aspects of running Docker have been added to other operating systems to support Docker there more efficiently too \citep{microsoft_linux_2019}.
The success even lead to standardisation and industry collaboration \citep{oci_open_2019}.

The key accomplishment of Docker as an "application" is to make a "bundled" aggregation of software (the so-called "image") available to any system equipped to run Docker, without requiring much else from the host besides the actual Docker application installation.
This is a rather attractive proposition and Docker's very easy to use user interface has lead to widespread adoption and use of Docker in a variety of domains, e.g., cloud computing infrastructure \citep[e.g.,][]{Bernstein2014}, data science \citep[e.g.,][]{boettiger_introduction_2015}, and edge computing \citep[e.g.,][]{alam_orchestration_2018}.
It provided to be a natural match for "cloud deployment" which runs, or at least appears to run, "seamlessly" without much explicit reference to the underlying machine, architecture or operating system: containers are portable and can be deployed with very little in terms of dependencies on the host system---only the container runtime is required.
Images are normally built from plain text documents called `Dockerfile`.
A `Dockerfile` has a specific set of instructions to create and document a well-defined environment, i.e., install specific software and expose specific ports.

For statistical computing and analysis centered around R, the Rocker&nbsp;Project has provided a variety of Docker containers since its start in 2014 \citep{RJ-2017-065}.
The Rocker&nbsp;Project provides several lines of containers spanning to from building blocks with `R-release` or `R-devel`, via containers with [RStudio&nbsp;Server](https://rstudio.com/products/rstudio/) and [Shiny&nbsp;Server](https://rstudio.com/products/shiny/shiny-server/), to domain-specific containers such as [`rocker/geospatial`](https://github.com/rocker-org/geospatial) \citep{rocker_geospatial_2019}.
These containers form _image stacks_, building on top of each other for better maintainability (i.e., smaller `Dockerfile`s), composability, and to reduce build time.
Also of note is a series of "versioned" containers which match the R release they contain with the _then-current_ set of packages via the MRAN Snapshot views of CRAN \citep{microsoft_cran_2019}.
The Rocker&nbsp;Project's impact and importance was acknowledged by the Chan Zuckerberg Initiative's _Essential Open Source Software for Science_, who provide findung for the projects's sustainable maintenance, community growth, and targeting new hardware platforms including GPUs \citep{chan_zuckerberg_initiative_maintaining_2019}.

Docker is not the only containerisation software.
An alternative stemming from the domain of high-perfomance computing is **Singularity** \citep{kurtzer_singularity_2017}.
Singularity can run Docker images, and in the case of Rocker works out of the box if the main process is R, e.g., in `rocker/r-base`, but does not succeed in running images where there is an init script, e.g., in containers that by default run RStudio&nbsp;Server.
In the latter case, a `Singularity` file, a recipe akin to a `Dockerfile`, needs to be used.
To date, no comparable image stack to Rocker exists on [Singularity Hub](https://singularity-hub.org/).
A further tool for running containers is [**podman**](https://github.com/containers/libpod), which also can build `Dockerfile`s and run Docker images.
Proof of concepts for using podman to build and run Rocker containers exist\footnote{See \href{https://github.com/nuest/rodman}{https://github.com/nuest/rodman} and \href{https://github.com/rocker-org/rocker-versioned/issues/187}{https://github.com/rocker-org/rocker-versioned/issues/187}}.
Yet the prevelance of Docker, especially in the broader user community beyond experts or niche systems, and the vast amount of blog posts and courses for Docker, currently caps specific development efforts for both Singularity and podman in the R community.
This might quickly change when usability and spread increase, or security features such as rootless/unprivileged containers, which both these tools support out of the box, become more sought after.

# Container images
\label{images}

## Images for alternative R engines
\label{alternatives}

As outlined above, R is a widely-used language with a large community.
The large number of extension packages provides access to an unrivaled variety of established and upcoming features.
Nevertheless, special use cases and experimental projects in academia and industry exist to test new approaches, or provide features different to what the R implementation maintained by the [R Core Team](https://www.r-project.org/contributors.html) and available via [CRAN](https://cran.r-project.org/) provides.
While the images presented in this section are far from being as vetted, stable, and widely used as any of the Rocker images, they demonstrate an important advantage of containerisation technology, namely the abiliy to transparently build portable stacks of open source software and make them easily accessible to others.
All these alternative implementations of R are published under GPL licenses.
Since all implementations of R claim better performance as a core motivation, a comparision based on Docker images, potentially leveraging the [resource restriction mechanisms](https://docs.docker.com/config/containers/resource_constraints/) of Docker to level the playing field, seems useful future work.

**Microsoft R Open** (MRO) is an R distribution formerly known as Revolution R Open (RRO) before Revolution Analytics was acquired by Microsoft.
MRO is compatible with main R and its packages and it "[..] includes additional capabilities for improved performance, reproducibility, and platform support." \citep{microsoft_mro}
Most notably these capabilities are the MRAN repository, which is enabled by default, and some bundled packages such as \CRANpkg{checkpoint} and specific R packages maintained by Microsoft\footnote{See "Bundled Packages" on \href{https://mran.microsoft.com/rro/installed}{https://mran.microsoft.com/rro/installed}.}.
Originally the optional integration with [Intel® Math Kernel Library](https://software.intel.com/en-us/mkl) (MKL) for multi-threading in linear algebra operations \citep{microsoft_multithread} was a distinguishing feature before it was made available publicly, and can be used in base R, too \citep{eddelbuettel_thinking_2018}.

MRO does not provide official Docker images, but a set of community-maintained `Dockerfile`s and Docker images are provided by the poject `mro-docker` ([https://github.com/nuest/mro-docker](https://github.com/nuest/mro-docker)), e.g., the image [`nuest/mro`](https://hub.docker.com/repository/docker/nuest/mro/).
The images are inspired by the Rocker images and can be used much in the same fashion, effectively a drop-in replacement allowing users to quickly evaluate if the benefits of MRO + Intel® MKL apply to their use case.
Version-tagged images are provided for the latest bugfix release of recent R versions.
Extended license information about MKL is printed at every startup.

**Renjin** is an interpreter for the R language running on top of the [Java Virtual Machine](https://en.wikipedia.org/wiki/Java_virtual_machine) (JVM) providing full two-way access between Java and R code \citep{wikipedia_renjin_2018}.
It was developed to combine the benefits of R, such scripting and extension packages, with the JVM's advantages in the areas of security, cross-platform availability and integration into enterprise platforms.
R extension packages need to be specially compiled and are distributed via the Java package manager [Apache Maven](https://en.wikipedia.org/wiki/Apache_Maven), cf. [http://packages.renjin.org/packages](http://packages.renjin.org/packages) for available packages.
Packages are loaded on demand, i.e., at the first call to `library()`.
Not all R packages, especially one linking to binary libraries, are available, e.g., `rgdal` \footnote{\href{http://packages.renjin.org/package/org.renjin.cran/rgdal/1.4-4/build/1}{http://packages.renjin.org/package/org.renjin.cran/rgdal/1.4-4/build/1}}.
There are no offical Docker images for Renjin, but community-maintained images for selected releases only are available under `nuest/renjin` on Docker&nbsp;Hub and GitHub at [https://hub.docker.com/r/nuest/renjin](https://hub.docker.com/r/nuest/renjin) and [https://github.com/nuest/renjin-docker](https://github.com/nuest/renjin-docker) respectively.
These images expose the command line interface of Renjin in a similar fashion as Rocker images and allow an easy evaluation of Renjin's suitability, but are not intended for production use.

**pqR** ([http://www.pqr-project.org/](http://www.pqr-project.org/)) is a "a pretty quick version of R".
_pqR_ attempts to improve R on some opinionated issues in the R language and is the basis for experimental features, e.g., automatic diffentiation\footnote{\href{https://riotworkshop.github.io/abstracts/riot-2019-pqr.txt}{https://riotworkshop.github.io/abstracts/riot-2019-pqr.txt}}.
The source code development [on GitHub](https://github.com/radfordneal/pqR/) is a one man project and it does not provide any Docker images.
But especially disruptive approaches may contribute to the development of the R ecosystem, so the `nuest/pqr` image was independently created, see Docker Hub at [https://hub.docker.com/r/nuest/pqr/](https://hub.docker.com/r/nuest/pqr/) and GitHub at [https://github.com/nuest/pqr-docker](https://github.com/nuest/pqr-docker).

**FastR** ([https://github.com/oracle/fastr](https://github.com/oracle/fastr)) is "A high-performance implementation of the R programming language, built on GraalVM" \citep{oracle_labs_oraclefastr_2020}.
It is developed by Oracle, connects R to the GraalVM ecosystem \citep{wikipedia_graalvm_2019}, and also claims superior performance but also targets full compatibility with base R \citep{oracle_labs_oraclefastr_2020}.
There are no offical Docker images provided, but indepedently maintained experimental images and `Dockerfile`s are provided by [nuest/fastr-docker](https://github.com/nuest/fastr-docker), e.g., [`nuest/fastr` on Docker Hub](https://cloud.docker.com/repository/docker/nuest/fastr/).

## Bioconductor
\label{bioc}

_Bioconductor_ ([https://bioconductor.org/](https://bioconductor.org/)) is an open source, open development project for the analysis and comprehension of genomic data \citep{gentleman_bioconductor_2004}.
The project consists of 1823 R software packages as of October 30th 2019, as well as packages containing annotation or experiment data.
_Bioconductor_ has a semi-annual release cycle, each release is associated with a particular version of R, and Docker images are provided for current and past versions of _Bioconductor_ for convenience and reproducibility.
All images, included  are described on the _Bioconductor_ web site (see [https://bioconductor.org/help/docker/](https://bioconductor.org/help/docker/)), created with `Dockerfile`s maintained on GitHub, and distributed through Docker&nbsp;Hub\footnote{See \href{https://github.com/Bioconductor/bioconductor_docker}{https://github.com/Bioconductor/bioconductor\_docker} and \href{https://hub.docker.com/u/bioconductor}{https://hub.docker.com/u/bioconductor} respectively.}.
_Bioconductor_'s 'base' Docker images are built on top of the `rocker/rstudio` image.
_Bioconductor_ installs packages based on the R version in combination with the Bioconductor version, and therefore uses Bioconductor version tagging `devel` and `RELEASE_X_Y`, e.g., `RELEASE_3_10`.
Past and current combinations of R and _Bioconductor_ will therefore be accessible via a specific image tags.
The _Bioconductor_ `Dockerfile` selects the desired version of R from Rocker, adds required system dependencies, and uses the \CRANpkg{BiocManager} package for installing appropriate versions of _Bioconductor_ packages.
A strength of this approach is that the responsibility for complex software configuration (including customized development) is shifted from the user to the experienced _Bioconductor_ core team.
A recent audit of the _Bioconductor_  image stack `Dockerfile` led to the deprecation of several community-maintained images, because the numerous specific images became too hard to understand, complex to maintain, and cumbersome to extent.
As part of the simplification, a recent innovation is to produce a `bioconductor_docker:devel` image to emulate the _Bioconductor_ nightly Linux build machine as closely as possible.
This image contains the build system environment variables and the _system dependencies_ needed to install and check almost all (1813 out of 1823) _Bioconductor_ software packages and saves users and package developers from managing these themselves.
Furthermore the image is configured so that `.libPaths()` has `/usr/local/lib/R/host-site-library` as the first location.
Users mounting a location on the host file system to this location can persist installed packages across Docker sessions or image updates.
Many R users pursue flexible work flows tailored to particular analysis needs, rather than standardized work flows.
The new `bioconductor_docker` image is well-suited for this pattern, while `bioconductor_docker:devel` provides developers with a test environment close to _Bioconductor_'s build system.

## Images for (historic) R versions
\label{versions}

As with any other software, each new version of R comes with its share of changes.
Some are breaking changes, some are not, but the fact is that running a piece of code in a given version might give different results from another version can be an issue when it comes to reproducibility of workflows and stability of applications.
For example, think about R random seed: starting with `R 3.6.0`, running `set.seed(2811); sample(1:1000, 2)` will not give the same result as if it was run inside an older version of R.
That might seem trivial, but all the code using the random number generator will not be exactly reproducible after this breaking change.
Therefore, controlling the version of software is most crucial for reproducible research \citep[e.g.][]{boettiger_introduction_2015}.

Containers are perfectly suited to capture a specific configuration of a computing environment to prohibit such problems.
The Rocker images provides the _versioned stack_ for different the version of R since the project inception, which use the _then-current_ stable Debian base image \citep[cf.][]{RJ-2017-065}.
Based on custom build phase hooks\footnote{\href{https://docs.docker.com/docker-hub/builds/advanced/}{https://docs.docker.com/docker-hub/builds/advanced/}}, i.e., small shell scripts executed at different phases in the automated build of Rocker images, there are also semantic version tags for the most recent freezed (i.e., using MRAN) versions with only the major and minor version\footnote{See \href{https://github.com/rocker-org/rocker-versioned/blob/master/VERSIONS.md}{https://github.com/rocker-org/rocker-versioned/blob/master/VERSIONS.md}, FIXME: \href{https://github.com/rocker-org/rocker-versioned/issues/42}{https://github.com/rocker-org/rocker-versioned/issues/42}.}.
So, at the time of writing this article, `rocker/r-ver:3` and `3.6` are aliases for `3.6.0`, because `3.6.1` is the latest release.
With the release of `3.6.2` and pinning of the MRAN version in `3.6.1`, the tags `3` and `3.6` will deliver the same image as `3.6.1`.
These tags allow users to update the base image to retrieve bugfixes while reducing the risk of also introducing breaking changes.

[r-online](https://github.com/ColinFay/ronline) is an app for helping users to detect breaking changes between different R versions, and for historic exploration of R.
With a standalone NodeJS app or [online](https://srv.colinfay.me/r-online), the user can compare a piece of code run in two separate versions of R.
Internally, r-online opens one or two Docker instances with the given version of R based on Rocker images, executes a given piece of code, and returns the result to the user.

A container also provides an isolated sandbox environment suitable for testing and evaluation, without interfering with the "main" working environment.
This enables cross version testing and bugfixing.
To take this even further, the Rocker contributors are discussing the provision of R versions reaching back futher than the project's own inception, reaching back as far as R `2.x` and `1.x` \footnote{\href{https://github.com/rocker-org/rocker-versioned/issues/138}{https://github.com/rocker-org/rocker-versioned/issues/138}}.
The main challenges are finding a suitable base image with a matching OS version, or making the adjustments to compile R on more recent OS releases.

## Windows Images
\label{windows}

Docker containers on the Windows operating system were originally quite cumbersome to use.
They required an extra tool, [Docker Toolbox](https://docs.docker.com/toolbox/), which by now only exists as a legacy solution for older Windows systems.
Docker toolbox leverages `docker-machine` (see also Section&nbsp;\nameref{interfaces} for an R package interfacing with `docker-machine`) to handle the process of creating a local virtual host which could host Docker Engine, while exposing the regular Docker CLI.
The Docker CLI commands are forwarded to the virtual host transparently for the user.

For current Windows Server (2016 and later) and Windows Desktop ([Docker Desktop](https://www.docker.com/products/docker-desktop) requires Windows 10) versions, Docker is supported natively\footnote{\href{https://www.docker.com/products/windows-containers}{https://www.docker.com/products/windows-containers}} and different base images are offered by Microsoft\footnote{\href{https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/index}{https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/index}}.
The Docker CLI can be used in just the same way as on other operating systems but not every base image is supported on every Windows host\footnote{\href{https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility}{https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility}}.
On Docker Desktop for Windows, the user can run both Linux-based and Windows-based containers, but only one of the Docker daemons can be used at a time\footnote{See "Switch between Windows and Linux containers" on \href{https://docs.docker.com/docker-for-windows/}{https://docs.docker.com/docker-for-windows/}.}.

**rocker-win** ([https://github.com/nuest/rocker-win](https://github.com/nuest/rocker-win)) is a proof of concept for running R in Windows-based containers \citep{nust_rocker-win_2019}.
It provides selected R versions using three different base images using `microsoft/windowsservercore` and `mcr.microsoft.com/windows/servercore`.
These base images match different Windows versions: Windows Server 2019, Windows Server 2016, and Windows Server, version 1803.
The images are built using two CI services, [Travis CI](https://docs.travis-ci.com/user/reference/windows/) and [Appveyor](https://www.appveyor.com/docs/windows-images-software/), which provide the different Windows versions and therefore support different base images, and published automatically [on Docker&nbsp;Hub](https://hub.docker.com/r/nuest/rocker-win).
The images are built from manually maintained seperate `Dockerfile`s, are tagged with both Windows-variant and R version, e.g., `nuest/rocker-win:ltsc2019-3.6.2` or `nuest/rocker-win:1803-latest`, and run the `R.exe` process by default.
All these images can be run on Docker for Desktop on Windows 10.
New R versions are only added on demand.
The explorative rocker-win project demonstrates the variety of Windows on Docker, which is potentially confusing, e.g., because of the different base images.
The proof of concept provides a starting point for any Windows-constrained user to leverage containerisation, including examples for apps using \CRANpkg{plumber}, \CRANpkg{Shiny}, or R packages with system dependencies such as \CRANpkg{sf}.
The latter is quite comfortable and fast actually, compared to the default installation from source on Linux, because CRAN ships pre-compiled binaries for Windows.
However, compared to the various Linux images, the footprint of Windows images is quite large.

For Windows, just as originally for Linux, the cloud use cases drive the development and Docker and Microsoft collaborate closely\footnote{\href{https://www.docker.com/partners/microsoft}{https://www.docker.com/partners/microsoft}}.
In most cases, individual developers or researchers worrying about reproducibility, will prefer the more widely and host independent Linux-based containers.
Naturally, a Windows license is required for the host machine, and the licensing impacts the potential to build images in clouds or redistribute exported images as archive files.
But the developments in the `rocker-win` prototype show that containers can also be used for R workflows depending on Windows-only tools, for leveraging an existing Windows Server-based infrastructure where policies can otherwise not be met, or for streamlining interactions with system operating staff that runs Windows-based containers themselves.

## Non-Rocker Linux images
\label{nondebian}

\label{rhub}
The **R-hub** project provides _"a collection of services to help R package development"_, with the package builder as the most prominent one\citep{r-hub_docs_2019}.
The builder allows R package developers to check their R package on different platforms and R versions using a web form or the package \CRANpkg{rhub} \citep{csardi_rhub_2019}.
The builder uses Docker containers to conduct these checks, taking advantages of their well-defined environments and sandboxing.
The `Dockerfile`s and images are published on Docker&nbsp;Hub\footnote{\href{https://hub.docker.com/u/rhub}{https://hub.docker.com/u/rhub}} and GitHub\footnote{\href{https://github.com/r-hub/rhub-linux-builders}{https://github.com/r-hub/rhub-linux-builders}} respectively.
The images comprise release and development builds of R running in the base images of the [Debian](https://www.debian.org/), [Ubuntu](https://ubuntu.com/), [Fedora](https://getfedora.org/), and [CentOS](https://centos.org/) linux distributions (Arch Linux is under development\footnote{\href{https://github.com/r-hub/rhub-linux-builders/pull/41}{https://github.com/r-hub/rhub-linux-builders/pull/41}}, see `rhub::local_check_linux_images()` for a full list), and are intended for debugging (see Section&nbsp;\nameref{development}) not for reproducible workflows.
_The rationale_ of not re-using the Rocker image stack is the high number of operatings systems and configurations, which can be more uniformly maintained in a seperate suite of images.
Also, a common structure across the respective image stacks of the operating systems, e.g., for `debian`, adding GCC in `debian-gcc` and then R's development version in `debian-gcc-devel`.
The platform also has a specific user configuration differing from the setup covering typical applications of Rocker images, and the images include considerably more software, such as a fairly complete LaTeX installation, to provide an experience closer to CRAN.

[**RStudio**](https://rstudio.com/) also maintains a set of [Docker images](https://github.com/rstudio/r-docker) for multiple operating systems including SUSE, Ubuntu, CentOS, and Debian.
The image stack includes an [opinionated R installation](https://github.com/rstudio/r-builds) and ensures the R installation and profile is consistent across the different Linux distributions.
The base image also installs R in a versioned directory and uses a minimal set of build and runtime dependencies.
Furthermore these images are responsible for creating pre-compiled binary R packages for Linux, which supplement the binaries built by CRAN.
These pre-compiled packages dramatically decrease the installation time of R packages on Linux and subsequently can decrease the build time of Docker images \citep{lopp_package_2019}.

There are also independent projects installing R or RStudio on [**Alpine Linux**](https://www.alpinelinux.org/), but not beyond the proof of concept stage\footnote{\href{https://gitlab.com/artemklevtsov/r-alpine}{https://gitlab.com/artemklevtsov/r-alpine} or \href{https://www.github.com/velaco/alpine-r}{https://www.github.com/velaco/alpine-r} provide different use case images but are not recently maintained (development of the latter documented in \citet{ratesic_building_2018}); \href{https://github.com/cmplopes/alpine-r}{https://github.com/cmplopes/alpine-r} has only base R images, is relatively up-to-date but sparsely documented; \href{https://github.com/CenterForStatistics-UGent/mountainr}{https://github.com/CenterForStatistics-UGent/mountainr} is a ghost project with only a single short `Dockerfile`, yet it shows the easy installation of the latest R from Alpine sources}.
In general, Alpine-based Docker images are often chosen for minimalistic and thereby small and secure containers, e.g., in hardware with limited storage.
R-hub's proof of concept [`r-minimal`](https://github.com/r-hub/r-minimal) takes this to the extreme with compressed image size of 20MB and bespoke install scripts, e.g., to cleverly remove compilers after package installation.
For R though, this advantage quickly deteriorates if more features are needed, e.g., if adding compilers, packages, or especially if installing powerful data science and communication tools, like RStudio or LaTeX.
Furthermore the distribution uses the [`musl` C library](https://www.musl-libc.org/) instead of the `glibc` used in Debian/Ubuntu, for which more tooling and experiences exist\footnote{Cf. \href{https://github.com/rocker-org/rocker/issues/231}{https://github.com/rocker-org/rocker/issues/231}}.
In fact, using Debian's 'slim' variant and removing capabilities, one could probalby achieve similarly small image sizes on a more common stack.

**\pkg{altRnative}** ([https://github.com/ismailsunni/altRnative/](https://github.com/ismailsunni/altRnative/)) is an experimental R package for running the same code across multiple containerised versions of R, intended for comparison across operating systems and different implementations of R.
It comes with a collection of `Dockerfile`s and corresponding images in multiple combinations, currently including, e.g., MRO, FastR, Fedora, and TERR\footnote{See `Dockerfile`s at \href{https://github.com/ismailsunni/dockeRs}{https://github.com/ismailsunni/dockeRs}.}.

## Data Science images
\label{datascience}

_Data Science_ is a widely discussed topic among all academic disciplines \citep[e.g.,][]{donoho_50_2017}.
The discussions shed a light on the tools and craftspersonship behind the analysis of data with computational methods.
The practice of Data Science often involves a combination tools and software stacks and requires a cross-cutting skillset.
This complexity and an inherent concern for openness and reproducibility in the Data Science community lead to Docker being used widely.
<!--A general description of what to consider when creating a Docker image used for data science projects in R is provided on [environments.rstudio.com](https://environments.rstudio.com/docker).-->
This section presents exemplary Docker images and image stacks featuring R intended for Data Science.

The [**Jupyter Docker Stacks**](https://github.com/jupyter/docker-stacks/) project are a set of ready-to-run Docker images containing Jupyter applications and interactive computing tools \citep{project_jupyter_jupyter_2018}.
The `jupyter/r-notebook` image includes R and "popular packages", and naturally also the IRKernel ([https://irkernel.github.io/](https://irkernel.github.io/)), an R kernel for Jupyter so that Jupyter Notebooks can contain R code cells.
R is also included in the catchall `jupyter/datascience-notebook` image\footnote{\href{https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html}{https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html}}.
For example, these images allow users to quickly start a Jupyter Notebook server locally or build their own specialised images on top of stable toolsets.
R is installed using the Conda package manager, which can manage environments for various programming languages, pinning both the R version and the versions of R packages\footnote{See \texttt{jupyter/datascience-notebook}'s \texttt{Dockerfile} at \href{https://github.com/jupyter/docker-stacks/blob/master/datascience-notebook/Dockerfile\#L47}{https://github.com/jupyter/docker-stacks/blob/master/datascience-notebook/Dockerfile\#L47}.}.

**Kaggle** provides the [`gcr.io/kaggle-images/rstats`](https://hub.docker.com/r/kaggle/rstats) image (previously `kaggle/rstats`) and [corresponding `Dockerfile`](https://github.com/Kaggle/docker-rstats) for usage in their Machine Learning competitions and easy access to the associated datasets.
It includes machine learning libraries such as Tensorflow and Keras (see also image `rocker/ml` in Section&nbsp;\nameref{rocker-gpu}), and also configures the \CRANpkg{reticulate} package.
The image uses a base image with _all packages from CRAN_, `gcr.io/kaggle-images/rcran`, which requires a Google Cloud Build as Docker&nbsp;Hub would time out\footnote{Originally, a stacked collection of over 20 images with automated builds on Docker~Hub was used, see \href{https://web.archive.org/web/20190606043353/http://blog.kaggle.com/2016/02/05/how-to-get-started-with-data-science-in-containers/}{https://web.archive.org/web/20190606043353/http://blog.kaggle.com/2016/02/05/how-to-get-started-with-data-science-in-containers/} and \href{https://hub.docker.com/r/kaggle/rcran/dockerfile}{https://hub.docker.com/r/kaggle/rcran/dockerfile}}.
The final extracted image size is over 25GB, which makes it debatable if having everything available is actually convenient.

```{bash kaggle_images, eval=FALSE, include=FALSE}
docker pull gcr.io/kaggle-images/rcran
docker pull kaggle/rcran # over 22 months old
docker pull gcr.io/kaggle-images/rstats
docker images | grep kaggle
```

As a further example, [**Radiant** project](https://radiant-rstats.github.io/docs/) provides several images, e.g., [`vnijs/rsm-msba-spark`](https://hub.docker.com/r/vnijs/rsm-msba-spark), for their browser-based business analytics interface based on \CRANpkg{Shiny} (`Dockerfile` [on GitHub](https://github.com/radiant-rstats/docker)) and for use in education as part of an MSc course.
As data science often applies a multitude of tools, this image favours inclusion over selection and features Python, Postgres, JupyterLab and Visual Studio Code besides R and RStudio, bringing the image size up to 9GB.

```{bash radiant_images, eval=FALSE, include=FALSE}
docker pull vnijs/rsm-msba-spark
docker images | grep vnijs
```

**Gigantum** ([http://gigantum.com/](http://gigantum.com/) is a platform for open and decentralized data science with a focus on using automation and user-friendly tools for easy sharing of reproducible computational workflows.
Gigantum builds on the _Gigantum Client_ (running either locally or on a remote server) for development and execution of data-focused _Projects_, which can be stored and shared via the _Gigantum Hub_ or via a zipfile export.
The Client is a user friendly interface to a backend using Docker containers to package, build, and run Gigantum projects.
It is configured to use a default set of Docker base images ([https://github.com/gigantum/base-images](https://github.com/gigantum/base-images)), and users are able to define and configure their own custom images.
The available images include two with R based on Ubuntu Linux have the [`c2d4u` CRAN PPA](https://launchpad.net/~marutter/+archive/ubuntu/c2d4u3.5/) pre-configured for installation of binary R packages\footnote{\href{https://docs.gigantum.com/docs/using-r}{https://docs.gigantum.com/docs/using-r}}.
The R images vary in the included authoring environment, i.e., Jupyter in `r-tidyverse` or both Jupyter \& RStudio in `rstudio-server`.
The independent image stack can be traced back to the Gigantum environment and its features.
The R images are based on Gigantum's `python3-minimal` image, originally to keep the existing front-end configuration, but also to provide a consistent Python-to-R interoperability.
The `Dockerfile`s also use build args to specify bases, for example for different version of NVIDIA CUDA for GPU processing\footnote{See \href{https://github.com/gigantum/base-images/blob/master/_templates/python3-minimal-template/Dockerfile}{https://github.com/gigantum/base-images/blob/master/\_templates/python3-minimal-template/Dockerfile} for the \texttt{Dockerfile} of \texttt{python3-minimal}.} so that appropriate GPU drivers can be enabled automatically when supported.
Furthermore, Gigantum's focus lies on environment management via GUI and ensuring a smooth user interaction, e.g., with reliable and easy conflict detection and resolution.
For this reason, project repositories store authoritative package information in a separate file per package, allowing Git to directly detect conflicts and changes.
A Dockerfile is generated from this description that inherits from the specified base image, and additional custom Docker instructions may be appended by users, though Gigantum's default base images do not currently include the `littler` tool, which is used by Rocker to install packages within `Dockerfile`s.
Because of these specifics, instructions from `rocker/r-ubuntu` could _not_ be readily re-used in this image stack (see Section&nbsp;\nameref{conclusions}).
Both approaches enable `apt` as an installation method, and this is exposed via the GUI-based environment management,\footnote{See \href{https://docs.gigantum.com/docs/environment-management}{https://docs.gigantum.com/docs/environment-management}}.
The image build and publication process is scripted with Python and JSON template configuration files, unlike Rocker which relies on plain `Dockerfile`s.
A minor reason in the inception of the images were also project constraints requiring a Rocker-incompatible licensing of the `Dockerfile`s, i.e., the MIT License.

# Use cases and applications
<!-- from packages closer to Docker to second level packages to more complex use cases -->
\label{applications}

## Interfaces for Docker in R
\label{interfaces}

Interfacing with the Docker daemon is typically done through the [Docker Command Line Interface](https://docs.docker.com/engine/reference/commandline/cli/) (Docker CLI).
However, moving back and forth between an R console and the command line can create friction in workflows and reduce reproducibility.
A number of first-order R packages provide a interface to the Docker CLI, allowing to automate interaction with the Docker CLI from an R console.

Each of these packages has particular advantages as they provide function wrappers for interacting with the Docker CLI at different stages of a container's life cycle.
Examples of such interactions are installing the Docker software, creating `Dockerfile`s (\CRANpkg{dockerfiler}, \pkg{containerit}), building images and launching a containers (\CRANpkg{stevedore}, \pkg{docker}) on a local machine or on the cloud.
As such, the choice of which package is most useful depends on the use-case at hand, but also the users level of expertise.

```{r clientfunctionalities, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
functionalities <- tribble(
  ~id, ~Functionality,
  "dockerfile", "Generate a Dockerfile",
  "build", "Build an image",
  "execute", "Execute a container locally or remotely",
  "cloud", "Deploy or manage an instances in the cloud",
  "interact", "Interact with an instance (e.g., file transfer)",
  "images", "Manage storage of images",
  "othercontainers", "Supports Docker and Singularity",
  "api", "Direct access to Docker API instead of using the CLI",
  "install", "Installing Docker software"
)

# some functionalities might be missing. please add them if you find that any are missing!
package_functionalities <- tribble(
  ~package, ~id,
  "stevedore", c("api", "execute", "cloud", "images", "interact"), # build?
  "dockyard", c("dockerfile", "build", "execute"), # cloud?
  "dockermachine", c("install", "execute", "cloud", "interact"),
  "AzureContainers", c("build", "execute", "cloud"),
  "babelwhale", c("execute", "interact", "othercontainers"),  
  "harbor", c("execute", "cloud", "images")
)

tab <- 
  full_join(
  functionalities %>% crossing(package = package_functionalities$package), 
  package_functionalities %>% unnest(id) %>% mutate(check = "\\checkmark"),#\u2714 # \\checkmark
  by = c("id", "package")
) %>%
  mutate(check = ifelse(is.na(check), "", check)) %>% 
  spread(package, check) %>% 
  slice(match(functionalities$id, id)) %>% 
  select(-id)

# inserting a table in the R Journal currently fails
# waiting for new release of articles including this PR to be published: https://github.com/rstudio/rticles/pull/261
# Until then, use remotes::install_github("rstudio/rticles")
if (packageVersion("rticles") >= "0.12.3") {
  requireNamespace("kableExtra")
  knitr::kable(tab, escape = FALSE) %>%
    kableExtra::row_spec(0, angle = -90)
} else {
  cat("\n\n[Functionality overview table omitted due to a small bug in rticles]\n\n")
}
```

**\pkg{harbor}** ([https://github.com/wch/harbor](https://github.com/wch/harbor) is not actively maintained anymore, but should be honorably mentioned as the first R package for managing Docker images and containers.
It uses the \CRANpkg{sys} package to run system commands against the Docker CLI, both locally and through an SSH connection, and has convenience functions, e.g., for listing and removing containers/images and for accessing logs.
The output of container executions are converted to appropriate R types.
The Docker CLI's basic functionality, while evolving quickly and with small concern for avoiding breaking changes, is unchanged for a long time so a core function such as `harbor::docker_run(image = "hello-world")` still works depsite the stopped development.
```{r harbor, eval=FALSE, include=FALSE}
remotes::install_github("wch/harbor")
harbor::docker_run(image = "hello-world")
```

**\CRANpkg{stevedore}** ([https://cran.r-project.org/package=stevedore](https://cran.r-project.org/package=stevedore)) is currently the most powerful Docker client in R.
It interfaces with the Docker daemon over the Docker HTTP API\footnote{\href{https://docs.docker.com/engine/api/latest/}{https://docs.docker.com/engine/api/latest/}} via a Unix socket on Linux or MacOS, over a named pipe on Windows, or over an HTTP/TCP connection.
The package is the only one not using system calls to the `docker` CLI for managing images and containers and easily exposes connections to remote Docker daemons, which has to be configured on the Docker level otherwise.
Using the API gives access to more information and is system independent and likely more reliable than parsing command line output.
\CRANpkg{stevedore}'s own interface is automatically generated based on the OpenAPI specification of the Docker daemon, but still similar to the Docker CLI.
The interface is similar to R6 objects, in that a `stevedore_object` has a number of functions attached to it that can be called, and multiple specific versions of the Docker API can be supported thanks to the automatic generation\footnote{See \href{https://github.com/richfitz/stevedore/blob/master/development.md}{https://github.com/richfitz/stevedore/blob/master/development.md}.}.

**\CRANpkg{AzureContainers}** is an interface to a number of container-related services in Microsoft's [Azure Cloud](https://azure.microsoft.com/) \citep{AzureContainers_2019}.
While it is mainly intended for working with Azure, as a convenience feature it includes lightweight, cross-platform shells to Docker and Kubernetes (tools `kubectl` and `helm`).
These can be used to create and manage arbitrary Docker images and containers, as well as Kubernetes clusters on any platform or cloud service.

**\CRANpkg{babelwhale}** allows executing and interacting with containers, which can use either Docker or Singularity as a backend \citep{cannoodt_babelwhale_2019}.
The package provides a unified interface to interact with Docker and Singularity containers.
Users can, for example, execute a command inside a container, mount a volume or copy a file.

**\pkg{dockyard}** ([https://github.com/thebioengineer/dockyard](https://github.com/thebioengineer/dockyard)) has the goal of lowering barrier to creating `Dockerfile`s, building Docker images, and deploying Docker containers.
The package follows the increasingly used piping paradigm of the `tidyverse` style of programming for chaining R functions representing the instructions in a `Dockerfile`.
An existing `Dockerfile` can be used as a template.
\pkg{dockyard} also includes wrappers for common steps, such as installing an R package or copying files, and build-in functions for building an image running a container, to make using Docker even more approachable to R users with a single API.

**\pkg{dockermachine}** ([https://github.com/cboettig/dockermachine](https://github.com/cboettig/dockermachine)) is an R package to provide a convenient interface to [Docker&nbsp;Machine](https://docs.docker.com/machine/overview/) from R.
The CLI tool `docker-machine` allows users to create and manage virtual host on local computers, local data centers, or at cloud providers.
A local Docker installation can be configured to transparntly forward all commands issued on the local Docker CLI to a selected (remote) virtual host.
Docker&nbsp;Machine was especially crucial for local use in early days of Docker, when no native support was available for Mac or Windows computers, but remains relevant for provisioning on remote systems.
The package has not received any updates for two years, but is functional with a current version of `docker-machine` (`0.16.2`).
It potentially lowers the barriers for R users to run containers on various hosts, if using the Docker&nbsp;Machine CLI directly is perceived as a barrier, or enables scripted workflows with remote processing.
```{r dockermachine, eval=FALSE, include=FALSE}
library("dockermachine")
dockermachine::machine_create(driver = "amazonec2")
# machine started: https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:sort=instanceId
# get config on command line: $ docker-machine env machine
# activate: $ $(docker-machine env test) does not work, because of "
# run container: docker run -it -p 80:80 nginx
# allow inbound HTTP traffic
# http://ec2-3-85-85-61.compute-1.amazonaws.com/ shows "Welcome to nginx"!
# Nice!
# > dockermachine::machine_rm()
# Instance shut down.
```

## Capture and create environments
\label{envs}

Several second order R packages attempt to make the process of creating Docker images and using containers for specific tasks, such as running tests or rendering reproducible reports, easier.
While authoring and managing an environment with Docker by hand is possible and feasible for experts\footnote{See, e.g., this tutorial by RStudio on how to manage environments and package versions and to ensure deterministic image builds with Docker: \href{https://environments.rstudio.com/docker}{https://environments.rstudio.com/docker}.}, the following examples show the power of automation when environments become too cumbersome or acquiring the skills is not possible in due course.
Especially _version pinning_, with packages \pkg{remotes} and \pkg{versions} for R or by using MRAN, and with system package managers for different operating systems, can greatly increase the reproducibility of built images and are common approaches.

**\pkg{dockerfiler}** ([https://github.com/ColinFay/dockerfiler/](https://github.com/ColinFay/dockerfiler/)) is an R package designed for building `Dockerfile`s straight from R.
A scripted creation of a `Dockerfile` enables iteration and automation, for example for packaging applications for deployment (see \nameref{deployment}).
Being scriptable from R developers can leverage the tools available in R to parse a `DESCRIPTION` file, to get system requirements, to list dependencies, versions, etc.
**\pkg{containerit}** ([https://github.com/o2r-project/containerit/](https://github.com/o2r-project/containerit/)) attempts to take this one step further and includes these tools to automatically create a `Dockerfile` that can execute a given workflow \citep{nust_containerit_2019}.
\pkg{containerit} accepts R `sessionInfo` objects as input and provides helper functions to derive these from workflows, e.g., an R script or R Markdown document, by analysing the session state at the end of the workflow.
It relies on the \pkg{sysreqs} ([https://github.com/r-hub/sysreqs/](https://github.com/r-hub/sysreqs/)) package and it's mapping of package system dependencies to platform specific installation package names\footnote{See \href{https://sysreqs.r-hub.io/}{https://sysreqs.r-hub.io/}.}.
\pkg{containerit} uses \CRANpkg{stevedore} to streamline the user interaction and improve the created `Dockerfile`s, e.g., by running a container for the desired base image to extract the already available R packages.
**\pkg{dockr}** ([https://github.com/smaakage85/dockr](https://github.com/smaakage85/dockr)) is a very similar package attempting to mirror a given R session, including local non-CRAN packages\citep{kjeldgaard_dockr_2019}.
Users can manually add statements for non-R dependencies to the `Dockerfile`.
**\CRANpkg{liftr}** aims to solve the problem of persistent reproducible reporting in statistical computing based on the R Markdown format \citep{xie2018} for dynamic documents \citep[\href{https://nanx.me/liftr/}{https://nanx.me/liftr/}, ][]{liftr2019}.
The irreproducibility of authoring environments can become an issue for collaborative documents and large-scale platforms for processing documents.
\CRANpkg{liftr} makes the document the main and sole workflow control file and the only file that needs to be shared between collaborators for consistent environments, e.g. demonstrated in the DockFlow project (https://dockflow.org).
It introduces new fields to the R Markdown document header, allowing users to manually declare the dependencies, including versions, for rendering the document.
The package then generates a `Dockerfile` from this metadata and provides a utitility function to render the document inside a Docker container, i.e., `render_docker("foo.Rmd")`.
An RStudio addin even allows compilation of documents with a single push of a button.

The package **\CRANpkg{renv}** ([https://rstudio.github.io/renv/](https://rstudio.github.io/renv/)) helps users to manage the state of the R library in a reproducible way, further providing isolation and portability \citep{renv2019}.
The package does not cover system dependencies, though, the \CRANpkg{renv}-based environment can be transferred into a container either by restoring the environment based on the main configuration file `renv.lock` or by storing the \CRANpkg{renv}-cache on the host and not in the container \citep{ushey_using_2019}.

## Using R to power enterprise software in production environments
\label{enterprise}

R has been historically viewed as a tool for analysis and scientific research, but not for creating software that corporations can rely on for production services.
However, thanks to advancements in R running as a web service, along with with the ability to deploy R in Docker containers, modern enterprises are now capable of having real-time machine learning powered by R.
A number of packages and projects enabled R to respond to client reqests over TCP/IP and local socket servers, such as \CRANpkg{Rserve}, \CRANpkg{svSocket}, [rApache](http://www.rapache.net) and more recently \CRANpkg{plumber} ([https://www.rplumber.io/](https://www.rplumber.io/)) and \pkg{RestRserve} ([http://restrserve.org](http://restrserve.org)), which even processes incoming requests in parallel with forked processes using \CRANpkg{Rserve}.
The latter two also provide documentation for deployment with Docker or ready to use automated builds of images\footnote{See \href{https://www.rplumber.io/docs/hosting.html\#docker}{https://www.rplumber.io/docs/hosting.html\#docker}, \href{https://hub.docker.com/r/trestletech/plumber/}{https://hub.docker.com/r/trestletech/plumber/} and \href{https://hub.docker.com/r/rexyai/restrserve/}{https://hub.docker.com/r/rexyai/restrserve/}.}.
These software allow other (remote) processes and programming languages to interact with R and to expose R-based function in a service architecture with HTTP APIs.
APIs based on these package can be deployed with scalability and high availability using containers.
This pattern of deploying code matches those used by software engineering services created in more established languages in the enterprise domain, such as Java or Python, and R can be used alongside those langages as a first class member of a software engineering technical stack.

CARD.com implemented a web application for the optimization of the acquisition flow and the real-time analysis of debit card transactions.
The software used \CRANpkg{Rserve} and rApache and was deployed in Docker containers.
The R session behind \CRANpkg{Rserve} acted as a read-only in-memory database, which was extremely fast and scalabale, for the many concurrent rApache processes responding to the live-scoring requests of various divisions of the company.
Similarly dockerized R scripts were responsible for the ETL processes and even the client-facing email, text message and push notification alerts sent in real-time based on card transactions.
The related Docker images were made available at [https://github.com/cardcorp/card-rocker](https://github.com/cardcorp/card-rocker).
The images extend `rocker/r-base` and additionally entailed an SSH client and a workaround for being able to mount SSH keys from the host, Pandoc, the Amazon Web Services (AWS) SDK, and Java, which is required by the AWS SDK.
The AWS SDK allowed to run R consumers reading from real-time data processing streams of [AWS Kinesis](https://aws.amazon.com/kinesis/) \footnote{See useR!2017 talk \href{https://static.sched.com/hosted\_files/user2017/2f/AWR Kinesis at useR 2017.pdf}{"Stream processing with R in AWS"}.}.
The applications were deployed on Amazon Elastic Container Service ([ECS](https://aws.amazon.com/ecs/)).
The main learnings from using R in Docker was the imporance of not only pinning the R package versions via MRAN, but also moving away from Debian testing to a distribution with long-term support.
For the use case at hand, this switch served the priority to have more control over upstream updates, and to minimize the risk of breaking the automated builds of the Docker images and production jobs.

The AI @ T-Mobile team created a set of neural network machine learning natural language processing models to help customer care agents manage text-based messages for customers \citep{t-mobile_enterprise_2018}.
For example, one model quickly identifies if a message is from a customer or not \citep[see \CRANpkg{Shiny}-based \href{https://secure.message.t-mobile.com/v1/shiny/is-customer/app/}{demo}, ][]{nolis_small_2019}, others tell which customers are likely to make a repeat purchase.
If a data scientist creates a machine learning model and exposes it through a \CRANpkg{plumber} API, then someone else on the marketing team could write software that sends different emails depending on that real-time prediction.
The models are convolutional neural networks that use the \CRANpkg{keras} package and run in a Rocker Docker image.
The corresponding `Dockerfile`s are published [on GitHub](https://github.com/tmobile/r-tensorflow-api).
Since the models power tools for agents and customers, they need to have extremely high uptime and reliability.
The AI @ T-Mobile team found that the models performed well and today these models power real-time services that are called over a million times a day.

## Deployment and continuous delivery
\label{deployment}

The cloud is the natural environment of containers, and subsequently containers are the go-to mechanism to deploy R server applications.
More and more continuous integration (CI) and continuous delivery (CD) services also use containers, opening up new options for use.
The controlled nature of containers, i.e., the possibility to abstract internal software environment from a minimal dependency outside of the container is crucial, for example to match test or build environments with production environments or transfer runnable entities to as-a-service infrastructures.

First, different packages use containers for the **deployment of R and \CRANpkg{Shiny} apps**.
\CRANpkg{Shiny} is a popular package for creating interactive online dashboards with R and it enables users with very diverse backgrounds to create stable and user friendly web applications.
For example, _ShinyProxy_ ([https://www.shinyproxy.io/](https://www.shinyproxy.io/)) is an open source tool to deploy Shiny apps in an enterprise context.
They feature single sign-on, but also in scientific use cases \citep[e.g., ][]{savini_epiexplorer_2019,glouzon_structurexplor_2017}.
ShinyProxy uses Docker containers to isolate user sessions and to achieve scalability for multi-user scenarious with multiple apps.
ShinyProxy itself is written in Java to accomodate corporate requirements and may itself run in a container for stability and availability.
The tool is build on ContainerProxy ([https://www.containerproxy.io/](https://www.containerproxy.io/)), which provides similar features for executign long-running R jobs or interactive R sessions.
The started containers can run on a regular Docker host but also in clusters.
Another example is the package \CRANpkg{golem} ([https://github.com/ThinkR-open/golem](https://github.com/ThinkR-open/golem)), which makes an heavy use of `dockerfiler` when it comes to creating the `Dockerfile` for building production-grade Shiny applications and deploying them.
\CRANpkg{googleComputeEngineR} ([https://cloudyr.github.io/googleComputeEngineR/](https://cloudyr.github.io/googleComputeEngineR/)) enables quick deployments of key R services, such as RStudio and Shiny, onto cloud virtual machines (VMs) with Google Cloud Compute Engine \citep{googleComputeEngineR_2019}.
The package utilises `Dockerfile`s to move the labour of setting up those services from the user to a premade Docker image, which is configured and run in the cloud VM.
For example, by specifying the template `template="rstudio"` in functions `gce_vm_template()/gce_vm()` an up to date RStudio Service image is launched for development work, while specifying `template="rstudio-gpu"` will launch an RStudio Server image with a GPU attached, etc.

Second, containers can be used to create **platform installation packages** in a DevOps setting.
The [OpenCPU](https://www.opencpu.org/) system provides an HTTP API for data analysis based on R.
\citet{ooms_opencpu_2017} describes how various platform-specific installation files for OpenCPU are created using Docker&nbsp;Hub: the automated builds install the software stack from source on different operating systems; afterwards a script file downloads the images and extracts the OpenCPU binaries.

Third, containers can greatly facilitate the **deployment to existing infrastructures**.
_Kubernetes_ ([https://kubernetes.io/](https://kubernetes.io/)) is a container-orchestration system for managing container-based application deployment and scaling.
A _cluster_ of containers, orchestrated as a single deployment, e.g., with Kubernetes, can mitigate limitations on request volumes or a container occupied with computationally intensive task.
A cluster features load-balancing, autoscaling of containers across numerous servers (in the cloud or on premise), and restarting failed ones.
It may be your organisation has a Kubernetes cluster already for other applications, or you may use one of the different provides for managed Kubernetes clusters.
Docker containers are used within Kubernetes clusters to hold native code, for which Kubernetes creates a framework around network connections and scaling of resources up and down.
R applications, big parallel tasks, or scheduled batch jobs can be deployed in a scalable way using Kubernetes, and the deployment can even be triggered by changes to code repositories (i.e., CD), see blog post "R on Kubernetes" \citep{edmondson_r_2018}.
The package \pkg{googleKubernetesR} ([https://github.com/RhysJackson/googleKubernetesR](https://github.com/RhysJackson/googleKubernetesR)) is a proof of concept for wrapping the Google Kubernetes Engine API, Google's hosted Kubernetes solution, in an easy to use R package.
The package \CRANpkg{analogsea} provides a solution to programmatically create and destroy cloud VMs on the [Digital Ocean](https://www.digitalocean.com/) platform \citep{analogsea_2019}.
It also includes R wrapper functions to install Docker in such a VM, manage images, and control containers straight from R functions.
These functions are translated to Docker CLI commands and transferred transparently to the respective remove machine using SSH.
\CRANpkg{AzureContainers} is an umbrella package providing interfaces three commercial services of Microsoft's Azure Cloud, namely [Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) for running individual containers, [Container Registry](https://azure.microsoft.com/en-us/services/container-registry/) for private image distribution, and [Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) for orchestrated deployments.
While a package like \CRANpkg{plumber} provides the infrastructure for turning an R workflow a service, for production purposes it is usually necessary to take into account scalability, reliability and ease of management.
AzureContainers provides an R-based interface to these features and thereby simplifies complex infrastructure management to a number of R function calls, given an Azure account with sufficient credit\footnote{See \emph{"Deploying a prediction service with Plumber"} vignette for details:  \href{https://cran.r-project.org/web/packages/AzureContainers/vignettes/vig01_plumber_deploy.html}{https://cran.r-project.org/web/packages/AzureContainers/vignettes/vig01\_plumber\_deploy.html}.}.

The prevelance of Docker in industry naturally leads to usage of containers with R in such settings as well, as customers already manage platforms in Docker containers.
These products often entail a high amount of open source software in combination with proprietary layers adding the relevant commercializable features.
One such example is RStudio's data science platform [RStudio Team](https://rstudio.com/products/team/).
It allows teams of data scientists and their respective IT/DevOps groups to develop and deploy code in R and Python around the RStudio Open Source Server inside of Docker images, without requiring users to learn new tools or directly interact with containers.
The best practices for [running RStudio with Docker containers](https://support.rstudio.com/hc/en-us/articles/360021594513-Running-RStudio-with-Docker-containers) as well as [Docker images](https://github.com/rstudio/rstudio-docker-products) for RStudio's commerical products are publicly available.

## Common or public work environments
\label{workenvs}

The fact that Docker images are portable and well defined make them useful when more than one person needs access to the same computing environment.
This is even more useful when some of the users do not have the expertise to create such an environment themselves, and when these environments can be run in public or shared infrastructure.

The [**Binder**](https://mybinder.readthedocs.io/en/latest/) project, maintained by the team behind Jupyter, makes it possible for users to **create and share computing environments** with others \citep{jupyter_binder_2018}.
A _BinderHub_ allows anyone with access to a web browser and an internet connection to launch a temporary instance of these custom environments and execute any workflows contained within.
From a reproducibility standpoint, Binder makes it exceedingly easy to compile a paper, visualize data, and run small examples from papers or tutorials without the need for any local installation.
To set up Binder for a project, a user typically starts at an instance of a BinderHub and passes the location of a repository with a workspace, e.g., a hosted Git repository, or a data repository like Zenodo.
Binder's core internal tool is `repo2docker`.
It deterministically builds a Docker image by parsing the contents of a repository, e.g., project dependency configurations or simple configuration files\footnote{See supported file types at \href{https://repo2docker.readthedocs.io/en/latest/config\_files.html}{https://repo2docker.readthedocs.io/en/latest/config\_files.html}.}.
In the most powerful case, `repo2docker` builds a given `Dockerfile`.
While this approach works well for most run of the mill Python projects, it is not so seamless for R projects.
This is partly because `repo2docker` does not support arbitrary base images due to the complex auto-generation of the `Dockerfile` instructions.
Two approaches make using Binder easier. for R users.
First, **\pkg{holepunch}** ([https://github.com/karthik/holepunch](https://github.com/karthik/holepunch)) is an R package that was designed to make sharing work environments accessible to novice R users based on Binder.
For any R projects that use the Tidyverse suite \citep{wickham_welcome_2019}, the time and resources required to build all dependencies from source can often time out before completion, making it frustrating for the average R user.
\pkg{holepunch} removes some of these limitations by leveraging Rocker images that contain the Tidyverse along special Jupyter dependencies, and only installs additional packages from CRAN and Bioconductor that are not already part of these images.
It short cicuits the configuration file parsing in `repo2docker` and starts with the Binder/Tidyverse base images, which eliminates a large part of the build time and in most cases results in a binder instance launching within a minute.
\pkg{holepunch} as a side effect also creates a `DESCRIPTION` file which then turns any project into a research compendium \citep{marwick_packaging_2018}.
The `Dockerfile` included with the project can also be used to launch a RStudio server locally, i.e., independent of Binder, which is especially useful when more or special computational resources can be provided there.
The local image usage reduces the number of seperately managed environments and thereby reduces work and increases portability and reproducibility.
Second, the **Whole&nbsp;Tale** project (https://wholetale.org) combines the strengths of the Rocker Project's curated Docker images with `repo2docker`.
Whole&nbsp;Tale is a National Science Foundation (NSF) funded project developing a scalable, open-source, multi-user platform for reproducible research \citep{brinckman2019, chard2019a}.
A central goal of the platform is to enable researchers to easily create and publish executable research objects\footnote{In Whole~Tale a _tale_ is a research object that contains metadata, data (by copy or reference), code, narrative, documentation, provenance, and information about the computational environment to support computational reproducibility} associated with published research \citep{chard2019b}.
Using Whole&nbsp;Tale, researchers can create and publish Rocker-based reproducible research objects to a growing number of repositories including DataONE member nodes, Zenodo and soon Dataverse.
Additionally, Whole&nbsp;Tale supports automatic data citation and is working on capabilities for image preservation and provenance capture to improve the transparency of published computational research artifacts \citep{mecum2018, mcphillips2019}.
For R users, Whole&nbsp;Tale extends the Jupyter Project's `repo2docker` component to simplify the customization of R-based environments for researchers with limited experience with either Docker or Git.
Multiple options have been discussed to allow users to change the base image used in `repo2docker` from the default Ubuntu LTS (long-term support) required to support the Rocker Project images. 
Whole&nbsp;Tale implemented a custom `RockerBuildPack`\footnote{See \href{https://github.com/whole-tale/repo2docker\_wholetale}{https://github.com/whole-tale/repo2docker\_wholetale}} to support customization of the `rocker/geospatial` image through `repo2docker` composability\footnote{Composability refers to the ability to combine multiple package managers -- such as R, `pip`, and `conda`}.
This works in part because Rocker images are based on a Debian distribution, so the instructions created by `repo2docker` for Ubuntu work because of compatible shell and package manager.

In **high-performance computing**, one use for containers is to run workflows on shared local hardware where teams manage their own high-performance servers.
This can follow one of several design patterns: users may deploy containers to hardware as a work environment for a specific project, conatiners may provide per-user persistent environments, or a single container can act as a common multi-user environment for a server.
In all cases, though, the containerized approach provides several advantages: First, users may use the same image and thus work environment on desktop and laptop computers, as well.
The former models provide modularity, while the latter approach is most similar to a simple shared server.
Second, software updates can be achieved by updating and redeploying the container, rather than tracking local installs on each server.
Third, the containerized environment can be quickly deployed to other hardware, cloud or local, if more resources are neccessary or in case of server destruction or failure.
In any of these cases, users need a method to interact with the containers, be it and IDE, or command-like access and tools such as SSH, which is usually not part of standard container recipes and must be added.
The Rocker&nbsp;Project provides containers pre-installed with the RStudio&nbsp;IDE.
In cases where users store nontrivial amounts of data for their projects, data needs to persist beyond the life of the container.
This may be via in shared disks, attached network volumes, or in separate storage where it is uploaded between sessions.
In the case of shared disks or network-attached volumes, care must be taken to persist user permissions, and of course backups are still neccessary.
When working with multiple servers, an automation framework such as [Ansible](https://www.ansible.com) may be useful for managing users, permisions, and disks along with containers.

\label{rocker-gpu}
Using **GPUs** (graphical processing units) as a specialised hardware from containerized common work environments is also possible and useful \citep{haydel_enhancing_2015}.
GPUs are increasingly popular for compute-intensive machine learning (ML) tasks, e.g., deep artificial neural networks \citep{schmidhuber_deep_2015}.
Though in this case, containers are not completely portable between hardware environments, but the software stack for ML with GPUs is so complex to set up that a ready-to-use container is helpful.
Containers running GPU software require drivers and libraries specific to GPU models and versions, and containers require a specialized runtime to connect to the underlying GPU hardware.
For NVIDIA GPUs, the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker) includes a specialized runtime plugin for Docker and a set of base images with appropriate drivers and libraries.
The Rocker&nbsp;Project [has a repository](https://github.com/rocker-org/ml) with (beta) images based on these that include GPU-enabled versions of machine-learning R packages, e.g., `rocker/ml` and `rocker/tensorflow-gpu`.

Teaching is a further example where shared computing environments and sandboxing can greatly improve the process.
First, **prepared environments for teaching** are especially helpful for courses that require access to a relatively complex setup of software tools, e.g., database systems.
R is very useful tool for interfacing with databases, because almost every open source and proprietary database system has an R package that allows users to connect and interact with the it.
This flexibility is even broadened by \CRANpkg{DBI}, which allows to create a common API for interfacing these databases, or the \CRANpkg{dbplyr} package, which runs \CRANpkg{dplyr} code straight against the database as queries.
But learning and teaching these tools comes with the cost of deploying or having access to an environment with the software and drivers installed.
For people teaching R, it can become a barrier if they need to install local versions of database drivers, or to connect to remote instances which might or might not be made available by IT services.
Giving access to a sandbox for the most common database environments is the idea behind [`r-db`](https://github.com/ColinFay/r-db), a Docker image that contains everything needed to connect to a database from R.
Notably, with `r-db`, the users don't have to install complex drivers or to configure their machine in a specific way.
The `rocker/tidyverse` base image ensures that users can also readily use packages for analysis, display, and reporting.
Second, the idea of a common environment and partitioning allow using **containers in teaching for secure execution and automated testing** of submissions by students.
[Dodona](https://dodona.ugent.be) is a web platform developed at Ghent University and is used to teach students basic programming skills, and it uses Docker containers to test submissions by students.
This means that both the code testing the students' submissions and the submission itself are executed in a predictable environment, avoiding compatibility issues between the wide variety of configurations used by students.
The containerization is also used to shield the Dodona servers from bad or even malicious code: memory, time and I/O&nbsp;limits are used to make sure students can't overload the system.
The web application managing the containers communicates with them by sending configuration information as a JSON document over standard input. 
Every Dodona Docker image shares a `main.sh` file that passes through this information to the actual testing framework, while setting up some error handling.
The testing process in the Docker containers sends back the test results by writing a JSON document to its standard output channel.
In June 2019, R support was added to Dodona using an image derived from the `rocker/r-base` image that sets up the `runner` user and and `main.sh` file expected by Dodona\footnote{\href{https://github.com/dodona-edu/docker-images/blob/master/dodona-r.dockerfile}{https://github.com/dodona-edu/docker-images/blob/master/dodona-r.dockerfile}}. 
It also installs the packages required for the testing framework and the exercises so that this doesn't have to happen every time a student's submission is evaluated.
The actual testing of R exercises is done using a custom framework loosely based on \CRANpkg{testthat}.
During the development of the testing framework it was found that the \CRANpkg{testthat} framework did not provide enough information to its reporter system to send back all the fields required by Dodona to render its feedback.
Right now, multiple statistics courses are developing exercises to automate the feedback for their lab classes.

**RCloud** ([https://rcloud.social](https://rcloud.social)) is a cloud-based platform for data analysis, visualisation and collaboration using R.
It provides a `rocker/drd` base image for easy evaluation of the platform\footnote{\href{https://github.com/att/rcloud/tree/master/docker}{https://github.com/att/rcloud/tree/master/docker}}.

## Processing
\label{processing}

The portability of containerized environments becomes particularly useful for improving expensive processing of data or shipping complex processing pipelines.
First, it is possible to **offload complex processing to a server** or clouds, and also parallel executing of processes for speeding up or serving of many users.
**\CRANpkg{batchtools}** provides a parallel implementation of Map for various schedulers \citep{Lang2017batchtools}.
The package can [schedule jobs with Docker Swarm](https://mllg.github.io/batchtools/reference/makeClusterFunctionsDocker.html)
**\CRANpkg{googleComputeEngineR}** has the function `gce_vm_cluster()` to create clusters of 2 or more virtual machines, running multi-CPU architectures.
Instead of running a local R script with the local CPU and RAM restrictions, the same code can be processed on all CPU threads of the cluster of machines in the cloud, all running a Docker container with the same R environments.
\CRANpkg{googleComputeEngineR} integrates with the R parralisation package \CRANpkg{future} to enable this with only a few lines or R code\footnote{\href{https://cloudyr.github.io/googleComputeEngineR/articles/massive-parallel.html}{https://cloudyr.github.io/googleComputeEngineR/articles/massive-parallel.html}}.
[Google Cloud Run](https://cloud.run) is a CaaS (Containers as a Service) platform.
Users can launch containers using any Docker image without worrying about underlying infrastructure in a so called serverless configuration.
The service takes care of network ingress, scaling machines up and down, authentication and authorisation---all features which are non-trivial for a developer to create on their own.
This can be used to scale up R code to millions of instances if need be with little or no changes to existing code, as demonstrated by the proof of concept `cloudRunR`\footnote{\href{https://github.com/MarkEdmondson1234/cloudRunR}{https://github.com/MarkEdmondson1234/cloudRunR}}, which uses Cloud Run to create a scalable R-based API using \CRANpkg{plumber}.
[Google Cloud Build](https://cloud.google.com/cloud-build/) and the Google Container Registry are a continuous integration service respectively image registry that offload building of images to the cloud, while serving the needs of commercial environments such as private Docker images or image stacks.
As Googl Cloud Build itself can run any container, the package \pkg{googleCloudRunner} demonstrates how R can be used as the control language for one time or batch processing jobs and scheduling of jobs\footnote{\href{https://code.markedmondson.me/googleCloudRunner/articles/cloudbuild.html}{https://code.markedmondson.me/googleCloudRunner/articles/cloudbuild.html}}.
\label{drake} **\CRANpkg{drake}** is a workflow manager for data science projects \citep{landau_drake_2019}.
It features implicit parallel computing and automated detection of the parts of the work that actually needs to be reexecuted.
\CRANpkg{drake} has been demonstrated to run inside containers for high reproducibility\footnote{See for example \href{https://github.com/joelnitta/pleurosoriopsis}{https://github.com/joelnitta/pleurosoriopsis} or \href{https://gitlab.com/ecohealthalliance/drake-gitlab-docker-example}{https://gitlab.com/ecohealthalliance/drake-gitlab-docker-example}, the latter even running in a CI, see~\nameref{cicd}.}, but also how to use \CRANpkg{future} package's function `makeClusterPSOCK()` to send parts of the workflow to a Docker image for execution\footnote{\href{https://docs.ropensci.org/drake/index.html?q=docker\#with-docker}{https://docs.ropensci.org/drake/index.html?q=docker\#with-docker}} \citep[see package's function documentation;~][]{future_2020}.
In the latter case, the container control code must be written by the user and the \CRANpkg{future} package ensures the host and worker can connect for communicating over socket connections.
**RStudio Server Pro** includes a functionality called [Launcher](https://solutions.rstudio.com/launcher/overview/) (since version&nbsp;1.2, released in 2019).
It gives users the ability to spawn R sessions and background/batch jobs in a scalable way on external clusters, e.g., [Kubernetes based on Docker images](https://support.rstudio.com/hc/en-us/articles/360019253393-Using-Docker-images-with-RStudio-Server-Pro-Launcher-and-Kubernetes) or [Slurm](https://slurm.schedmd.com/) clusters, and optionally, with Singularity containers.
A benefit of the proprietary Launcher software is the ability for R and Python users to leverage containerisation's advantages in RStudio without writing specific deployment scripts or learning about Docker or managing clusters at all.

\label{pipelines}
Second, containers are perfectly suited for **packaging and executing software pipelines** and required data.
Containers allow building complex processing pipelines that are independent from the host programming language.
Due to its original use case (see&nbsp;\nameref{introduction}), Docker has no standard mechanisms for chaining containers together; it lacks definitions and protocols for environment variables, volume mounts and/or ports that could enable transfer of input (parameters and data) and output (results) to and from containers.
Some packages, e.g., \pkg{containerit}, do provide Docker images that can be used similarly to CLIs, but their usage is cumbersome\footnote{\href{https://o2r.info/containerit/articles/container.html}{https://o2r.info/containerit/articles/container.html}}.
**\pkg{outsider}** ([https://docs.ropensci.org/outsider/](https://docs.ropensci.org/outsider/)) tackles the problem of integrating external programs into an R workflow \citep{bennett_outsider_2020}.
Installation and usage of external programs can be difficult, convoluted and even impossible if the platform is incompatible.
Therefore \pkg{outsider} uses the platform-independent Docker images to encapsulate processes in _outsider modules_.
Each outsider module has a `Dockerfile` and an R package with functions for interacting with the encapsulated tool.
Using only R functions, an end-user can install a module with the \pkg{outsider} package and then call module code to integrate a tool into their own R-based workflow seamlessly.
The \pkg{outsider} package and module manage the containers and handle the transmission of arguments and the transfer of files to and from a container.
These functionalities also allow a user to launch module code on a remote machine via SSH, expanding the potential computational scale.
Outsider modules can be hosted code-sharing services, e.g., on GitHub, and \pkg{outsider} contains discovery functions for them.
<!-- informative examples of how modules work, just as a reminder for @nuest
https://github.com/DomBennett/om..astral/blob/master/examples/astral.R
https://github.com/DomBennett/om..raxml#partitioned-dna-analysis
-->

## Packaging research reproducibly
\label{compendia}

Containers provide a high degree of isolation that is often desirable when attempting to capture a specific computational environment so that others can reproduce and extend a research result.
Many computationally intensive research projects depend on specific versions of original and third-party software packages in diverse languages, joined together to form a pipeline through which data flows.
New releases of even just a single piece of software in this pipeline can break the entire workflow, making it difficult to find the error and difficult for others to reuse existing pipelines.
These breakages can make the original the results irreproducible and not extandable.
The chance of a substantial disruption like this is high in a multi-year research project where key pieces of third-party software may have several major updates over the duration of the project.
The classical "paper" article is insufficient to adequately communicate the knowledge behind such research projects \citep[cf.][]{donoho_invitation_2010,marwick_how_2015}.

\citet{gentleman_statistical_2007} coined the term **Research Compendium** for a dynamic document together with supporting data and code.
They use the R package system \cite{core_writing_1999} as for the functional prototype to structuring, validation, and distribution of research compendia.
This concept has been taken up and extended\footnote{See full literature list at \href{https://research-compendium.science/}{https://research-compendium.science/}.}, not the least by applying containerisation and other methods for managing computing environments---see Section&nbsp;\nameref{envs}.
Containers give the researcher an isolated environment to assemble these research pipelines with specific versions of software.
Research workflows in containers are safe from contamination from other activities occuring on the researcher's computer, for example the installation of newest version of packages for teaching demonstrations.
Given the users in this scenario, i.e., often academics with limited formal software development training, templates and assistance with containers around research compendia is essential.
In many fields we see that a typical unit of research for a container is a research report or journal article, where the container holds the compendium, or self-contained set of data (or connections to data elsewhere) and code files needed to fully reproduce the article \citep{marwick_packaging_2018}.
The package \pkg{rrtools} ([https://github.com/benmarwick/rrtools](https://github.com/benmarwick/rrtools)) provides a template and convienence functions to apply good practices for research compendia, including a starter `Dockerfile`.
Images of compendium containers can be hosted on services such as Docker&nbsp;Hub for convienient sharing among collaborators and others. 
Similarly, packages such as \pkg{containerit} and \pkg{dockerfiler} can be used to manage the `Dockerfile` to be archived with a compendium on a data repository (e.g. [Zenodo](https://zenodo.org/), [Dataverse](https://dataverse.org/), [Figshare](https://figshare.com/), [OSF](https://osf.io/)).
A typial compendium's `Dockerfile` will pull a rocker image fixed to a specific version of R, and install R packages from the MRAN repository to ensure the package versions are tied to a specific date, rather than the most recent version.
A more extreme case is the _dynverse_ project ([https://dynverse.org/](https://dynverse.org/)), which packages over 50 computational methods with different environments (R, Python, C++, etc.) in Docker images, which can be executed from R.
_dynverse_ uses a CI (cf.&nbsp;\nameref{cicd}) to build Rocker-derived containers, test them, and push them to Docker&nbsp;Hub if the tests succeed.

Future researchers can download the compendium from the repository and run the included `Dockerfile` to build a new image that recreates the computational environment used to produce the original research results.
If building the image fails, the human-readable instructions in a `Dockerfile` are the starting point for rebuilding the environment.
Further safeguarding practices are currently under development or not part of common practice yet, such as preservation of images \citep{emsley_framework_2018} and storing them alongside `Dockerfile`s \citep[cf.][]{nust_opening_2017}, or pinning of system libraries beyond the tagged base images, which may be seen as stable or dynamic depending on the applied time scale \citep[see discussion on `debian:testing` base image in][]{RJ-2017-065}.
A recommendation of the recent National Academies' report on _Reproducibility and Replicability in Science_ is that journals _"consider ways to ensure computational reproducibility for publications that make claims based on computations"_ \citep{NASEM2019}.
In fields such as Political Science and Economics, journals are increasingly adopting policies requiring authors to publish the code and data required to reproduce computational findings reported in published manuscripts, subject to independent verification \citep{Jacoby2017,Vilhuber2019,Alvarez2018,Christian2018,Eubank2016,King1995}.
Problems with the computational environment, installation and availability of software dependencies are common.
R is gaining popularity in these communities, such as creating a research compendium.
In a sample of 105 replication packages published by the _American Journal of Political Science_ (AJPS) over 65% use R.
The NSF-funded Whole Tale project uses the Rocker Project community images with the goal of improving the reproducibility of published research artifacts and simplifying the publication and verification process for both authors and reviewers by reducing errors and time spent specifying the environment.

## Development, debugging, and testing
\label{development}

Containers can also serve as useful playgrounds to create environments ad-hoc or to provide very specific environments that are not needed or not easily available in day-to-day development for the purposes of developing R packages.
These environments may have specific versions of R, of R extension packages, and of system libraries used by R extension packages, and all of the above in a specific combination.

First, such containers can greatly facilitate **fixing bugs and quick evaluation**, because developers and users can readily start and later discard a container to investigate a bug report or try out a piece of software \citep[cf.][]{ooms_opencpu_2017}.
The container does not affect their regular system.
Using the Rocker images with RStudio, these disposable environments lack no development comfort (cf. Section&nbsp;\nameref{compendia}).
\citet{ooms_opencpu_2017} describes how `docker exec` can be used to get a root shell in a container for cusotmization during software evaluation.
\citet{eddelbuettel_debugging_2019} describes an example how a Docker container was used to debug an issue with a package only occuring with a particular version of Fortran, and using tools which are not readily available on all platforms (e.g., not on macOS).

Second, the strong integration of **system libraries in core packages** in the [R-spatial community](https://www.r-spatial.org/) makes containers essential for stable and proactive development of common classes for geospatial data modelling and analysis. For example, GDAL \citep{gdal_2019} is a crucial library in the geospatial domain.
GDAL is a system dependency allowing R packages such as \CRANpkg{sf}, which provides the core data model for geospatial vector data, or \CRANpkg{rgdal}, to accomodate users to be able to read and write hundreds of different spatial raster and vector formats.
\CRANpkg{sf} and \CRANpkg{rgdal} have hundreds of indirect reverse imports and dependencies and therefore the maintainers spend a lot of efforts not to break these.
Purpose built Docker are used to prepare for upcoming releases of system libraries, individual bug reports, and for the lowest supported versions of system libraries\footnote{Cf. \href{https://github.com/r-spatial/sf/tree/master/inst/docker}{https://github.com/r-spatial/sf/tree/master/inst/docker}, \href{https://github.com/Nowosad/rspatial_proj6}{https://github.com/Nowosad/rspatial\_proj6}, and \href{https://github.com/r-spatial/sf/issues/1231}{https://github.com/r-spatial/sf/issues/1231}}.
```{r geospatial_deps, eval=FALSE, include=FALSE}
length(tools::dependsOnPkgs(c("rgdal", "sf"), dependencies = c("most"), recursive = TRUE))
```

Third, there are special purpose images for identifying problems beyond the mere R code, such as **debugging R memory problems**.
The images significantly reduce the barrier to follow complex steps for fixing memory allocation bugs \citep[cf. Section~4.3 in][]{core_writing_1999}.
These problems are hard to debug and critical, both because when they occur they lead to fatal crashing processes.
[`rocker/r-devel-san`](https://github.com/rocker-org/r-devel-san) and [`rocker/r-devel-ubsan-clang`](https://github.com/rocker-org/r-devel-san-clang) are Docker images have a particularly configured version of R to trace such problems with gcc and clang compilers, respectively \citep[cf.~\CRANpkg{sanitizers} for examples,][]{eddelbuettel_sanitizers_2014}.
The image [`wch/r-debug`](https://github.com/wch/r-debug) is a purpose built Docker image with _multiple_ instrumented builds of R, each with a different diagnostic utility activated.

Fourth, containers are useful for **testing** R code during development.
To submit a package to CRAN, an R package must work with the development version of R, which must be compiled locally.
That can be a challenge for some users.
The R-hub service (see Section&nbsp;\nameref{rhub}) makes it easy to ensure that no errors occur, but to fix errors a local setup is still often warranted, e.g., using the image `rocker/r-devel`, and to test packages with native code, which can make the process more complex \citep[cf.][]{eckert_building_2018}.
The R-hub Docker images can also be used to debug problems locally using various combinations of Linux platforms, R versions, and compilers\footnote{See \href{https://r-hub.github.io/rhub/articles/local-debugging.html}{https://r-hub.github.io/rhub/articles/local-debugging.html} and \href{https://blog.r-hub.io/2019/04/25/r-devel-linux-x86-64-debian-clang/}{https://blog.r-hub.io/2019/04/25/r-devel-linux-x86-64-debian-clang/}}.
The images go beyond the configurations, or _flavours_, used by CRAN for checking packages\footnote{\href{https://cran.r-project.org/web/checks/check_flavors.html}{https://cran.r-project.org/web/checks/check\_flavors.html}}, e.g., with CentOS-based images, but lack a container for checking on Windows or OS X.
The images greatly support package developers to provide support on operating systems they are not familiar.
The package \pkg{dockertest} ([https://github.com/traitecoevo/dockertest/](https://github.com/traitecoevo/dockertest/)) is a proof of concept for automatically generating `Dockerfile`s and building images specifically to run tests\footnote{\pkg{dockertest} is not actively maintained, but mentioned still because of its interesting approach.}.
These images are accompanied with a special launch script so the tested source code is not stored in the image but the currently checked in version from a local Git repository is cloned into the container at runtime.
This approach clearly seperates test environment, test code, and current working copy of the code.
\label{rselenium} Another use case where a container helps to standardise tests across operating systems is detailed the vignettes of the package \CRANpkg{RSelenium} \citep{rselenium_2019}.
The package recommends Docker for running the [Selenium](https://selenium.dev/) Server application needed to execute test suites on browser-based user interfaces and webpages, but requires users to manually manage the Docker containers.

Fifth, Docker images can be used **on CI platforms** to streamline the testing of packages.
\citet{ye_docker_2019} describes how they speed up the process of testing by running tasks on [Travis&nbsp;CI](https://travis-ci.org/) within a container using `docker exec`, e.g., the package check or rendering of documentation.
\citet{cardozo_faster_2018} saved time, also on Travis&nbsp;CI, by re-using the testing image as the base for an image intended for publication on Docker&nbsp;Hub.
[`r-ci`](https://github.com/ColinFay/r-ci) is in turn used with [GitLab&nbsp;CI](https://docs.gitlab.com/ee/ci/), which itself is built on top of Docker images: the user specifies a base Docker image, and the whole tests are run inside this environment.
The `r-ci` image stack combines `rocker` versioning and a series of tools specifically designed for testing in a fixed environment with a customized list of preinstalled packages.
Especially for long running tests or complex system dependencies, these approaches to seperate installation of build dependencies with code testing streamline the development process.
Not due to a concern about time, but to control the environment used on a CI server, even this manuscript is rendered into a PDF and deployed to a GitHub-hosted website with every change (see `.travis.yml` and `Dockerfile` in the manuscript repository).
This gives on the one hand easy access after every update of the R Markdown source code, and on the other hand a second controlled environment making sure that the article renders successfully and correctly.

# Conclusions
\label{conclusions}

This article is a snapshot of the R-corner in a universe of applications built with a many-faced piece of software, Docker.
`Dockerfile`s and Docker images are the go-to methods for collaboration between roles in an organisation, such as development and IT operations teams, and between parties in the communication of knowledge, such as research workflows or education.
Docker became synonymous with applying the concept of containerisation to solve challenges of reproducible environments, e.g., in research and in development \& production, and of scalable deployments with the ability to move processing between machines easily (e.g., locally, one cloud providers VM, another cloud provider's Container-as-a-Service).
Reproducible environments, scalability and efficiency, and portability across platforms/infrastructures are the common themes behind R packages, use cases, and applications in this work.

The R packages and use cases presented show the growing number of users, developers, and real-world applications in the community and the resulting innovations.
But the applications also point to the challenge of keeping up with a continuously evolving landscape.
The use cases contributed by co-authors also have a degree of overlap, which can be expected as a common language and understanding of good practices is still taking shape.
Also, the ease with which one can create a complex software systems with Docker, such as an independent Docker image stack, to serve one's specific needs leads to parallel developments.
<!-- Tyler W./DC from Gigantum: ideas about ease-of-DIY together with difficulty of composition; shared starting points of Rocker -->
This ease-of-DIY in combination with the difficulty to compose environments based on `Dockerfile`s alone is a further reason for **fragmentation**.
Instructions can be outsourced into distributable scripts and then copied into the image during build, but that make `Dockerfile` hard to read and adds a layor of complexity.
Despite the different image stacks presented here, the pervasiveness of Rocker can be traced back to its maintainers and the user community valueing collaboration and shared starting points over impulses to create individual solutions.
Aside from that, fragmentation may not be a bad sign, but instead a reflection of a growing market, which is able to sustain multiple related efforts.
With the maturing of core building blocks, such as the Rocker suite of images, more systems will be built successfully but will also be behind the curtains.
Docker alone, as a flexible core technology, is not a feasible level of collaboration and abstraction.
Instead, the use cases and applications observed in this work provide a more useful division.

Nonetheless, at least on the level of R packages some **consolidation** seems in order, e.g., to reduce the number of packages creating `Dockerfile`s from R code or controlling the Docker daemon with R code.
It remains to be seen which approach to control Docker, via the Docker API as \pkg{stevedore} or via system calls as \pkg{dockyard}/\pkg{docker}/\pkg{dockr}, is more sustainable, or if the question will be answered by the endurance of maintainers and sufficient funding.
Similarly, the capturing of environments and their serialization in form of a `Dockerfile` currently happens at different levels of abstraction and re-use of functionality seems reasonable, e.g., \pkg{liftr} could generate the environment with \pkg{containerit}, which in turn may use \pkg{dockerfiler} for low level R objects representing a `Dockerfile` and its instructions.
In this consolidation, the Rocker&nbsp;Project could play the role of coordinating entity.
Though for the moment, the sign of the times points to more experimentation and feature growth, e.g., images for GPU-based computing and artificial intelligence.
Even with coding being more and more accepted as a required, and achievable skill, an easier access, for example by exposing containerisation benefits via simple user interfaces in the users' IDE, could be an important next step.
Currently containerisation happens more in the background at the system level.
Furthermore, a maturing of the Rockerverse packages for managing containers may lead to their adoption where currently manual coding is required, e.g. in the case of \CRANpkg{RSelentium} or \CRANpkg{drake} (see Sections&nbsp;\nameref{rselenium} and \nameref{drake} respectively).
In specific cases, e.g., for \CRANpkg{analogsea}, the interaction with the Docker daemon may remain to specific to reuse first order packages to control Docker.

New features, which make complex workflows accessible and reproducible, and the variety in packages connected with containerisation, even when they have overlapping features, are a signal and a support for a growing user base.
This growth is possibly the most important goal for the foreseeable future in the _Rockerverse_, and just like the Rocker images have matured over years of use and millions of runs, the new ideas and prototypes will have to proof themselves.
It should be noted that the dominant position of Docker is a blessing and a curse for these goals.
It could be wise to start experimenting with non-Docker containerisation tools now, e.g., R packages interfacing with other container engines such as [podman/buildah](https://github.com/containers/libpod) or [rkt](https://coreos.com/rkt/), or an R package for creating `Singularity` files.
Such efforts can help to avoid lock-in and to design sustainable workflows based on concepts of _containerisation_, not on their implementation in Docker.
If adoption of containerisation and R continue to grow, the missing pieces for a success predominantly lie in (a) coordination and documentation of activities to reduce repeated work in favour of open collaboration, (b) the sharing of lessons learned from use cases to build common knowledge and language, and (c) a sustainable continuation and funding for all of development, community support, and education.
A concrete effort to work towards these pieces is to sustain the structure and captured status quo from this work in form of a _CRAN Task View on containerization_.

# Author contributions

The ordering of authors following DN and DE is alphabetical.
DN conceived the article idea, \href{https://github.com/nuest/rockerverse-paper/issues/3}{initialised the formation of the writing team}, wrote sections not mentioned below, and revised all sections.
DB wrote the section on \pkg{outsider}.
GD contributed the CARD.com use case.
DE wrote the introduction and the section about Containerization and Rocker and reviewed all sections.
RC \& EH contributed to the section on interfaces for Docker in R (_dynverse_ and `dynwrap`).
DC contributed content on Gigantum.
ME contributed to the section on processing and deployment to cloud services.
CF wrote paragraphs about \pkg{r-online}, \pkg{dockerfiler}, \pkg{r-ci} and \pkg{r-db}.
SL contributed content on RStudio's usage of Docker.
BM wrote the section on research compendia and made the project Binder-ready.
HN \& JN co-wrote the section on the T-Mobile use case.
KR wrote the section about \pkg{holepunch}.
NR wrote paragraphs about shared work environments and GPUs.
LS \& NT wrote the section on Bioconductor.
CW wrote the sections on Whole Tale and contributed the publication reproducibility audit use case.
NX contributed content on \pkg{liftr}.
All authors approved the final version.
This articles was collaboratively written at \href{https://github.com/nuest/rockerverse-paper/}{https://github.com/nuest/rockerverse-paper/}.
The \href{https://github.com/nuest/rockerverse-paper/graphs/contributors}{contributors page}, \href{https://github.com/nuest/rockerverse-paper/commits/master}{commit history}, and \href{https://github.com/nuest/rockerverse-paper/issues/}{discussion issues} provide a detailed view on the respective contributions.

# Acknowledgements

DN is supported by the project Opening Reproducible Research ([o2r](https://www.uni-muenster.de/forschungaz/project/12343)) funded by the German Research Foundation (DFG) under project number [PE&nbsp;1632/17-1](https://gepris.dfg.de/gepris/projekt/415851837).
The funders had no role in data collection and analysis, decision to publish, or preparation of the manuscript.
CW is supported by the Whole Tale projects (https://wholetale.org) funded by the US National Science Foundation (NSF) under award [OAC-1541450](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1541450).

```{r word_stats, eval=FALSE, include=FALSE}
# devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
wordcountaddin::readability(here::here("manuscript.Rmd"))
wordcountaddin::text_stats(here::here("manuscript.Rmd"))
```

\bibliography{RJreferences}
